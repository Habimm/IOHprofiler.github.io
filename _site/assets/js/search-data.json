{
  "0": {
    "id": "0",
    "title": "",
    "content": "404 Page not found :( The requested page could not be found.",
    "url": "http://localhost:4000/404/",
    "relUrl": "/404/"
  },
  "1": {
    "id": "1",
    "title": "Benchmark",
    "content": "Benchmark PBO_suite : 23 pseudo-Boolean Problems.",
    "url": "http://localhost:4000/Benchmark/",
    "relUrl": "/Benchmark/"
  },
  "2": {
    "id": "2",
    "title": "Examples",
    "content": "Examples This page introduces two examples of implementing algorithms by C and Python. (1+1) Evolutionary algorithm by C IOHexperimenter provides an example interface in user_algorithm.c, Users can implement their own algorithms by replacing content in the file. There are values of three variables to be modified by users in the file. BUDGET_MULTIPLIER is about the maximal budget for each run of experiments, which is equal to dimension * BUDGET_MULTIPLIER， and INDEPENDENT_RESTARTS is the number of independent runs for each problem. For example, static const size_t BUDGET_MULTIPLIER = 50; static const size_t INDEPENDENT_RESTARTS = 1; static const uint32_t RANDOM_SEED = 1; The algorithm implementation must be done in user_Algorithm() function, the arguments of the function is void User_Algorithm(evaluate_function_t evaluate, const size_t dimension, const size_t number_of_objectives, const int *lower_bounds, const int *upper_bounds, const size_t max_budget, IOHprofiler_random_state_t *random_generator) where evaluation is a function to evaluate individuals, random_generator is a parameter for random generators supplied by the tool, and other arguments are useful information for users. An example of (1+1) evolutionary algorithm is as below. int *parent = IOHprofiler_allocate_int_vector(dimension); int *offspring = IOHprofiler_allocate_int_vector(dimension); int *best = IOHprofiler_allocate_int_vector(dimension); double *y = IOHprofiler_allocate_vector(number_of_objectives); double best_value; size_t i, j, l; size_t number_of_parameters = 3; double *p = IOHprofiler_allocate_vector(number_of_parameters); int hit_optimal = 0; int lambda = 1; double mutation_rate = 1/(double)dimension; generatingIndividual(parent,dimension,random_generator); p[0] = lambda; p[1] = 0.0; p[2] = 0.0; set_parameters(number_of_parameters,p); evaluate(parent,y); CopyIndividual(parent,best,dimension); best_value = y[0]; for (i = 1; i &lt; max_budget; ) { for(j = 0; j &lt; lambda; ++j){ CopyIndividual(parent,offspring,dimension); l = mutateIndividual(offspring,dimension,mutation_rate,random_generator); p[0] = lambda; p[1] = mutation_rate; p[2] = (double)l; set_parameters(number_of_parameters,p); evaluate(offspring, y); ++i; if(i == max_budget) { break; } if(if_hit_optimal()) { hit_optimal = 1; break; } if(y[0] &gt; best_value){ best_value = y[0]; CopyIndividual(offspring,best,dimension); } } if(hit_optimal) { break; } CopyIndividual(best,parent,dimension); } IOHprofiler_free_memory(parent); IOHprofiler_free_memory(offspring); IOHprofiler_free_memory(best); IOHprofiler_free_memory(p); IOHprofiler_free_memory(y); where mutation rate is fixed, and for each generation an offspring is generated by a mutate operator mutateIndividual and l is the number of flipped bits in mutation. Then offspring are evaluated by evaluate(offspring,y), y is a vector stores objectives. Every time that evaluate function is invoked, users can call set_parameters(number_of_parameters,p) to store some parameters values into log files. Please make sure that number_of_parameters is equal to the length of p. if_hit_optimal() returns true if the optimal of the problem has been found. Random search by Python IOHexperimenter provides an example interface in user_algorithm.py, Users can implement their own algorithms by replacing content in the file. There are values of three variables to be modified by users in the file. budget is about the maximal budget for each run of experiments, which is equal to dimension * budget， and independent_restart is the number of independent runs for each problem. For example, independent_restart = 1 budget = 2 The algorithm implementation must be done in user_algorithm() function, the arguments of the function is def user_algorithm(fun,lbounds,ubounds,budget) where fun is the class for testing problem, lbounds and uboundsare arrays for bounds of variables, and budget is maximal evaluations. An example of random search is as below. lbounds, ubounds = np.array(lbounds), np.array(ubounds) dim, best_value = fun.dimension, 0 while budget &gt; 0: X = lbounds + (ubounds - lbounds + 1) * np.random.rand(1, dim) X = X.astype(int); para = np.array([budget]) fun.set_parameters(para) F = [fun(x) for x in X] if fun.number_of_objectives == 1: index = np.argmin(F) if best_value is None or F[index] &gt; best_value: best_value = F[index] budget -= 1 return best_value A random bit string x is generated in each generation, and fun(x) returns the corresponding fitness value. Before calling fun(x), fun.set_parameters() is used to store an array of para, which means values of parameters will be logged. To know more configurations about the experiments, please visit configurations for C and Python.",
    "url": "http://localhost:4000/IOHexperimenter/Examples/",
    "relUrl": "/IOHexperimenter/Examples/"
  },
  "3": {
    "id": "3",
    "title": "Getting Started",
    "content": "Getting Started Getting started by C Getting started by Python Getting started by R Getting started by C See here for the first steps and execute the python do.py build-c To run your experiment using the tool, following files are necessary: IOHprofiler.h Headfile for source code. IOHprofiler.c source code for the project. readcfg.c, configuration.ini Configs of problems. user_experiment.c An interface to assign problems for the experiment. user_algorithm.c the file including implementation of the algorithm to be tested. Makefile.in With these files, you can invoke make to compile, then run your experiment. To specify the experiment, you need to take care of Configuration and Implementation of Algorithms. Configuration Configuration.ini consists of three parts: [suite], [observer] and [triggers]. [suite] is the session that collects problems to be tested in the experiment. suite_name: Currently, ONLY PBO suite is avaiable, please do not modify the value of suite_name, unless a new suite is created. function_id: presents id of problems of the suite. The format of function_id can be 1,2,3,4 using comma , to separate problems’ id, or be 1-4 using an en-dash - to present the range of problems’ id. instances_id: presents id of instances. Instanes 1 means there is no transformer operations on the problem. For instances 2-50, XOR and SHIFT operations are applied on the problem. For instances 5-100, SIGMA and SHIFT operations are applied on the problem. Larger instances ID will be considered as 1. dimensions: presents dimensions of problems. The format of dimensions is as 500,1000,1500. [observer] is about the setting of output files. observer_name: Currently, ONLY PBO observer is avaiable, please do not modify the value of observer_name, unless a new observer is created. result_folder: Directory where stores output files. algorithm_name: used for .info files. algorithm_info: user for .info files. parameters_name: names for recording parameters in algorithms. [triggers] are parameters for different output files, see documentation to know technique of recording evluations. number_target_triggers, base_evaluation_triggers: are for .tdat files. complete_triggers: is for .cdat files. number_interval_triggers: is for .idat files. Implementation of Algorithm user_algorithm.c includes the implementation of algorithms to be tested. Take the exiting user_algorithm.c as an example, a random local search algorithm is implemented in the function User_Algorithm(). static const size_t BUDGET_MULTIPLIER = 50; configs the maximal budget for evaluations as dimension * 50. static const size_t INDEPENDENT_RESTARTS = 1; configs that the algorithm will be tested once for each problem. evalute(parent,y) returns the fitness of parent to y. The size of parent should be equal to the dimension of the problem. To test your algorithm, please replace the content of User_Algorithm(). Getting started by Python See here for the first steps and execute the python do.py build-python To run your experiment using the tool, following files are necessary: readcfg.cfg, configuration.ini Configs of problems. user_experiment.py An interface to assign problems for the experiment. user_algorithm.py the file including implementation of the algorithm to be tested. With these files, you can invoke python experiment.py to run your experiment. To specify the experiment, you need to take care of Configuration and Implementation of Algorithms. Configuration Configuration.ini consists of three parts: [suite], [observer] and [triggers]. [suite] is the session that collects problems to be tested in the experiment. suite_name: Currently, ONLY PBO suite is avaiable, please do not modify the value of suite_name, unless a new suite is created. function_id: presents id of problems of the suite. The format of function_id can be 1,2,3,4 using comma , to separate problems’ id, or be 1-4 using an en-dash - to present the range of problems’ id. instances_id: presents id of instances. Instanes 1 means there is no transformer operations on the problem. For instances 2-50, XOR and SHIFT operations are applied on the problem. For instances 5-100, SIGMA and SHIFT operations are applied on the problem. Larger instances ID will be considered as 1. dimensions: presents dimensions of problems. The format of dimensions is as 500,1000,1500. [observer] is about the setting of output files. observer_name: Currently, ONLY PBO observer is avaiable, please do not modify the value of observer_name, unless a new observer is created. result_folder: Directory where stores output files. algorithm_name: used for .info files. algorithm_info: user for .info files. parameters_name: names for recording parameters in algorithms. [triggers] are parameters for different output files, see documentation to know technique of recording evluations. number_target_triggers, base_evaluation_triggers: are for .tdat files. complete_triggers: is for .cdat files. number_interval_triggers: is for .idat files. Implementation of Algorithm user_algorithm.py includes the implementation of algorithms to be tested. Take the exiting user_algorithm.py as an example, a random local search algorithm is implemented in the function User_Algorithm(). budget = 50 configs the maximal budget for evaluations as dimension * 50. independent_restart = 1 configs that the algorithm will be tested once for each problem. best_value = fun(parent) returns the fitness of parent to best_value. The size of parent should be equal to the dimension of the problem. fun.set_parameters(para) sets the parameters (para) to be logged in output files. If you don’t want to record paramters, just erase the statement. If you set parameters, it will be better to configure names of paramters in configuration.ini with the same order. To test your algorithm, please replace the content of user_algorithm(). Getting Started by R If devtools is not yet installed, please first use install.packages(&#39;devtools&#39;) Error messages will be shown in your R console if there is any installation issue. Now, the IOHexperimenter package can be installed and loaded using the following commands: devtools::install_github(&#39;IOHprofiler/IOHexperimenter@R&#39;) library(&#39;IOHexperimenter&#39;) This will install the package and all required dependencies. Getting Started Install the package using one of the methods mentioned package above. Create you own algorithm, or use the example algorithm provided in the documentation accessed by: ?benchmark_algorithm Note that your algorithm will need to accept exactly one parameter: An IOHproblem object, which contains the following information about the current problem: dimension function_id instance fopt (if known) xopt (if known) And the following functions: obj_func() target_hit() set_parameters() Run the benchmarks using the function ‘benchmark_algorithm’ Using the analyzer by visiting IOHAnalyzer page",
    "url": "http://localhost:4000/IOHexperimenter/GettingStarted/",
    "relUrl": "/IOHexperimenter/GettingStarted/"
  },
  "4": {
    "id": "4",
    "title": "Graphic User Interface",
    "content": "Graphic User Interface A web-based GUI is implemented in IOHanalyzer such that the user can analyze the experiment data easily in the web browser. using two R packages: shiny and plotly. In this section, we shall describe this Graphical User Interface (GUI) in detail. Moreover, for the users who are not familiar with R , a free GUI server is hosted online(http://iohprofiler.liacs.nl/) (For any bug report, feedback or requested change, the users are encouraged to contact us through iohprofiler@liacs.leidenuniv.nl). Alternatively, invoking the GUI is also straightforward on your local machine, given package IOHanalyzer is already installed: &gt; runServer() Loading required package: shiny Listening on http://127.0.0.1:3943 which will start the GUI server on the local machine (hence using IP address 127.0.0.1) and a random port number. The web browser will be launched and connect to this address immediately after starting the server. There are tour groups of functionalities on the page. Upload Data: This section provides functionality to upload experiment results (the format of which is regulated in Sec.~ ref{subsec:data-format}) or load an ``official’’ data set that is provided by the author. Fixed-Target Results This section provides statistics covering the fixed-target perspective of performance evaluation. That is, the results in this section mainly address the question about the statistical property of running time (i.e., function evaluations) that is needed to obtain a solution of a desired target quality. Fixed-Budget Results This sectioncovers the fixed-budget perspective, that is the statistics on objective function values obtained by the search points a given budget of function evaluations. In other words, the results in this section mainly address the question how good the search points are that a user can expect to see within a given frame of running time. Algorithm Parameters This section provides details about the evolution of the algorithm parameters that the user specified to be tracked in the experimental procedure. Data loading The GUI interface to load the experiment data is shown in the following figure, in which the user is asked to upload a compressed archive. The following compression format are supported: *.zip, *.bz, *.tar, *.xz, *.gz. Note that, when the user’s data set is enormous to handle, it is possible to speed up the uploading (and hence plotting) procedure by toggling option Efficient mode on, in which a subset is taken from the huge data set. Moreover, when using the online version of GUI (http://iohprofiler.liacs.nl/), the user can also load official data sets provided by the author, using the Load Data from Repository box on the right of the page. At the time of writing, two official data sets are made available, each of which contains results of $11$ algorithms on all $23$ test functions, over dimensions $16, 100, 625$. For the specification of those two data sets and updates on the data set, the user is suggested to visit https://github.com/IOHprofiler/IOHdata. Fixed-Target Results The fixed-target section has four different subsections: Data Summary Expected Runtime Probability Mass Function Cumulative Distribution. The analysis provided in those sections are described as follows: Data Summary provides some statistics on the running time $T(A, f, d, v)$, meaning the function evaluation an algorithm $A$ would require to reach target value $v$ of test function $f$ on dimension $d$. In the following, the indexing parameter in $T$ will be dropped if no ambiguity is created. Assuming a number of independent runs of algorithm $A$ is performed on the tuple of $(f, d, v)$, the set of results from all runs $(t_i)_i$ is considered as a simple random sample of $T$. Here, three tables are provided to summarize the sample $(t_i)_i$ of $T$: Data Overview A screenshot of this table is given as below. As counterintuitive as it may seem, this table contains the overview of the function value observed in a data set. The main reason of showing this table here is to provide the user a quick summary of the range of function value, which is required to play with the following two functionalities. The detailed explanation on columns of this table is given in Sec.~ ref{subsec:data_summary}. Runtime Statistics at Chosen Target Values A screenshot of this table is given as below. The table is obtained from The user can set the range and the granularity of the results in the box on the left. The table shows fixed-target running times for evenly spaced target values. More precisely, for each tuple of (algorithm $A$, target value $v$, dimension $d$) the table provides 1) successful runs: the number of runs (sample points) of algorithm $A$ in which at least one solution $x$ satisfying $f(x)&gt;v$ has been found 2) sample mean, median, standard deviation 3) sample quantiles: $Q_{2 %}, Q_{5 %}, ldots, Q_{98 %}$ and 4) the emph{expected running time} (ERT) as defined in Eq.~ eqref{eq:ERT}. Additionally, the user can also download this table in CSV format, or as a LaTeX{}~table. Original Runtime Samples The user interface is similar to the previous except that the sample points ${t_i}_{i}$ of running time $T(A, f, d, v)$ is shown here. Moreover, the user can choose between a long (all sample points are stored in a column) and a wide format (all sample points are stored in a row) for the table. Expected Runtime An interactive plot (using shiny package) illustrates the fixed-target running times. An example of this plot is shown as below as below. The interactive plot can be adjusted by a couple of options on the left menu as shown in figure, including showing/hiding mean and/or median values along with standard deviations and scaling axis logarithmically. The user also selects the algorithms to be displayed, the range of target values within which the curves are drawn. The displayed curves can be switched on and off by clicking on the legend on the right of the plot. In addition, the figure can be downloaded in the following format: pdf, eps, svg and png (which also applies to all the plots hereafter). Probability Mass Function For a selected target value $v$, the histogram of the running time, as displayed below, shows the number of runs $i$ where the running time falls into a given interval $[t,t+1)$, namely $t le t_i(A,f,d,v) &lt; t+1$. The bin size $[t,t+1)$ is automatically determined according to the so-called Freedman-Diaconis rule~ citep{Freedman1981}, which is based on the interquartile range of sample $(t_i)_i$. The user has two options: overlayed display, where all algorithms are displayed in the same plot separated one, where each algorithm is displayed in an individual sub-plot. In addition to the histogram, the empirical probability mass function (See figure below) might be helpful to get a finer look at the shape of the empirical distribution of $T(A,f,d,v)$. The user can opt to show all sample points ${t_i}_{i}$ for each algorithm (which will be plotted as dots), or only the empirical probability mass function itself. It is important to point out that the probability mass function is estimated in a ““continuous’’ manner, where running time samples are considered as $ mathbb{R}$-valued and then a Kernel Density Estimation (KDE) is taken to obtain the curve. Note also that in the figure many sample points seem overlapping, this might be caused by turning (in the data upload part) the ‘‘efficient mode’’ on, in which the raw data set is trimmed. Cumulative Distribution The empirical cumulative distribution function of the running time are computed for target values specified by the user. In addition to showing ECDFs for a single target value, it is recommended to aggregate ECDFs over multiple target, to obtain an overall performance profile for all algorithms. Such a functionality is exemplified below a set of evenly spaced target value can be generated by specifying the range and step of the target value. In this example, with the following setup, $f_{ min}=0.46$, $f_{ max}=4.91$, and $ Delta f=0.5$, the ECDF curves for target values $0.46,0.96,1.46, ldots, 4.91$ are computed. The aggregation across targets is defined in the following sense: for a set of target values $V$, $r$ number of independent runs on each function, the aggregated ECDF considers running time samples of all target values and runs together. In the upper figure, for algorithm $(30,30)$-vGA (blue curve) this is the case for around $70 %$ of the pairs after $t=2 ,0000$ function evaluations. For algorithm RLS (purple curve) the fraction is $80 %$. Ideally, the best algorithm would sample the maximal function value $f_{ max}$ in the first function evaluation. This algorithm would have a 100 % cumulative probability for any running time. In practice, such an algorithm does not exist, but it serves as a theoretical upper bound. Furthermore, this way of performing the aggregation can be leveraged to a set of test functions, namely it is straightforward to define the aggregated profile for a range of functions This functionality is also materizalized in IOHanalyzer . In the example figure, a table of pre-calculated target values are provided for each test function while all $23$ test functions are considered here by default. This table could also be customized by 1) downloading the current table (in a CSV-like format), 2) then modifying it according to user’s preference and 3) finally uploading the modified table again. The plot on the right will be re-computed upon uploading a new table of targets. Note that, please keep the format of the example table while editing it.",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/GraphicUserInterface/",
    "relUrl": "/IOHanalyzer(Post-Processing)/GraphicUserInterface/"
  },
  "5": {
    "id": "5",
    "title": "IOHanalyzer",
    "content": "IOHanalyzer In this page, an example on IOHanalyzer is given based on the benchmark data generated in the previous section. In case it occurs to be time-consuming for the reader to execute the benchmarking example, the exactly same data set can also be downloaded from the site. We provide and maintain two versions of the IOHanalyzer package: a CRAN (Comprehensive R Archive Network) version R package a latest stable version that is hosted on Github page. The CRAN version can be obtained by: &gt;install.packages(&#39;IOHanalyzer&#39;) To install the latest stable version, pkg{devtools} is required again: &gt;devtools::install_github(&#39;IOHprofiler/IOHanalyzer&#39;) And please import the package after installation: &gt;library(&#39;IOHanalyzer&#39;) Note that those two versions only differ in some aesthetic aspects of the plotting method, which undergos a continuous and constant improvement. The example here is mainly two-fold: the usage of the programming interface and the Graphical User Interface.",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/",
    "relUrl": "/IOHanalyzer(Post-Processing)/"
  },
  "6": {
    "id": "6",
    "title": "IOHexperimenter",
    "content": "IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics IOHprofiler is a new tool for analyzing and comparing iterative optimization heuristics. Given as input algorithms and problems written in C, C++, or Python*, it provides as output an in-depth statistical evaluation of the algorithms’ fixed-target and fixed-budget running time distributions. In addition to these performance evaluations, IOHprofiler also allows to track the evolution of algorithm parameters, making our tool particularly useful for the analysis, comparison, and design of (self-)adaptive algorithms. IOHprofiler is a ready-to-use software. It consists of two parts: IOHExperimenter, which generates the running time data; and IOHanalyzer, which produces the summarizing comparisons and statistical evaluations. Currently IOHExperimenter is built on the COCO software, but a new C++ based version is developing and will be released soon. [This code] implements the experimentation tool of IOHprofiler. For the analyzer part, please visit IOHanalyzer page. Requirements The experimentation of IOHprofiler has been tested with: gcc 5.4.1 python 2.7.12 R 3.4.4 and above (should also work on other versions) For a machine running the anylizing process, Visit IOHanalyzer page Getting Started Check out the Requirements above. 1a. To use the R-version of the IOHexperimenter, please look at the R-package branch of this repository link 1b. For the other versions of the IOHexperimenter, Download IOHprofiler experimentation code from github link and unzip the zip file, or type git clone https://github.com/IOHprofiler/Experimentation.git (git needs to be installed) In a system shell, cd into the Experimenter folder, where the file do.py can be found. execute the following statement: python do.py `options` Available options are: build-c Builds the C module. Two files, IOHprofiler.c and IOHprofiler.h, will be generated at code-experiment/build/c run-c Builds the C module and runs as an example some experiments in C. build-python Builds the python module. A python package named IOHprofiler will be installed run-python Builds the python module and runs as an example some experiments in python. Using the analyzer by visiting IOHanalyzer page",
    "url": "http://localhost:4000/IOHexperimenter/",
    "relUrl": "/IOHexperimenter/"
  },
  "7": {
    "id": "7",
    "title": "Problems",
    "content": "Problems Definition F1: OneMax (Hamming Distance） It asks to optimize $OM:{0,1} rightarrow [0..n], x mapsto sum_{i=1}^n{x_i}$. The problem has a very smooth and non-deceptive fitness landscape. Due to the well-known coupon collector effect, it is relatively easy to make progress when the function values are small, and the probability to obtain an improving move decreases considerably with increasing function value. F2: LeadingOnes The problem asks to maximize the function $LO:{0,1}^n to [0..n], x mapsto max {i in [0..n] mid forall {j} le {i}: x_j=1} = sum_{i=1}^n{ prod_{j=1}^i{x_i}}$, which counts the number of initial ones. F3: A Linear Function with Harmonic Weights The problem is a linear function $f:{0,1}^n to mathbb{R}, x mapsto sum_{i} i x_i$ with harmonic weights. F4-F17: The W-model The W-model comprises 4 basic transformations, each coming with different instances. We use $W( cdot, cdot, cdot, cdot)$ to denote the configuration chosen in our benchmark set. Reduction of dummy variables $W(k, ast, ast, ast)$: a reduction mapping each string $(x_1, ldots, x_n)$ to a substring $(x_{i_1}, ldots, x_{i_k})$ for randomly chosen, pairwise different $i_1, ldots, i_k in [n]$. Neutrality $W( ast, mu, ast, ast)$: The bit string $(x_1, ldots,x_n)$ is reduced to a string$(y_1, ldots,y_m)$ with $m:=n/ mu$, where $ mu$ is a parameter of the transformation. For each $i in [m]$ the value of $y_i$ is the majority of the bit values in a size-$ mu$ substring of $x$. More precisely, $y_i=1$ if and only if there are at least $ mu/2$ ones in the substring $(x_{(i-1) mu+1},x_{(i-1) mu+2}, ldots,x_{i mu})$. When ${n/ mu} notin { mathbb{N}}$, the last bits of $x$ are simply copied to$y$. In our assessment, we regard only the case $ mu=3$. Epistasis $W( ast, ast, nu, ast)$ The idea of epistasis is to introduce local perturbations to the bit strings. To this end, a string $x=(x_1, ldots,x_n)$ is divided into subsequent blocks of size $ nu$. Using a permutation $e_{ nu}:{0,1}^{ nu} to {0,1}^{ nu}$, each substring $(x_{(i-1) nu+1}, ldots,x_{i nu})$ is mapped to another string $(y_{(i-1) nu+1}, ldots,y_{i nu})=e_{ nu}((x_{(i-1) nu+1}, ldots,x_{i nu}))$. The permutation $e_{ nu}$ is chosen in a way that Hamming-1 neighbors $u,v in {0,1}^{ nu}$ are mapped to strings of Hamming distance at least $ nu-1$. In our evaluation, we use $ nu=4$ only. Fitness perturbation $W( ast, ast, ast,r)$. With this transformation we can determine the ruggedness and deceptiveness of a function. Unlike the previous transformations, this perturbation operates on the function values, not on the bit strings. To this end, a emph{ruggedness} function $r:{f(x) mid {x} in {0,1}^n }=:V to {V}$ is chosen. The new function value of a string $x$ is then set to $r(f(x))$ , so that effectively the problem to be solved by the algorithm becomes ${r} circ {f}$. We use the following three ruggedness functions. $r_1:[0..s] to [0.. lceil{s/2} rceil+1$ with $r_1(s)= lceil {s/2} rceil +1$ and $r_1(i)= lfloor {i/2} rfloor+1$ for $i&lt;s$ and even s, and $r_1(i)= lceil {i/2} rceil+1$ for $i&lt;s$ and odd $s$. This function maintains the order of the search points $r_2:[0..s] to [0..s]$ with $r_2(s)=s$, $r_2(i)=i+1$ for $i equiv {s {mod} 2}$ and $i&lt;s$, and $r_2(i)= max{ {i-1,0}}$ otherwise. This function introduces moderate ruggedness at each fitness level. $r_3:[0..s] to [0..s]$ with $r_3(s)=s$ and $r_3(s-5j+k)=s-5j+(4-k)$ for all $j in {[s/5]}$ and $k { in} [0..4]$ and $r_3(k)=s - (5 lfloor {s/5} rfloor - 1 )- k$ for $k in [0..s - 5 lfloor {s/5} rfloor -1]$. With this function the problems become quite deceptive, since the distance between two local optima implies a difference of $5$ in the function values. F4-F17 present superpositions of individual W-model transformations to the OneMax (F1) and the LeadingOnes (F2) problem. Precisely, F4-F17 are F4: $ text{OneMax} + W([n/2],1,1,id)$ F5: $ text{OneMax} + W([0.9n],1,1,id)$ F6: $ text{OneMax} + W([n], mu=3,1,id)$ F7: $ text{OneMax} + W([n],1, nu=4,id)$ F8: $ text{OneMax} + W([n],1,1,r_1)$ F9: $ text{OneMax} + W([n],1,1,r_2)$ F10: $ text{OneMax} + W([n],1,1,r_3)$ F11: $ text{LeadingOnes} + W([n/2],1,1,id)$ F12: $ text{LeadingOnes} + W(0.9n,1,1,id)$ F13: $ text{LeadingOnes} + W([n], mu=3,1,id)$ F14: $ text{LeadingOnes} + W([n],1, nu=4,id)$ F15: $ text{LeadingOnes} + W([n],1,1,r_1)$ F16: $ text{LeadingOnes} + W([n],1,1,r_2)$ F17: $ text{LeadingOnes} + W([n],1,1,r_3)$ F18: Low Autocorrelation Binary Sequences (LABS) The Low Autocorrelation Binary Sequences (LABS) problem poses a non-linear objective function over a binary sequence space, with the goal to maximize the reciprocal of the sequence’s autocorrelation: $ frac{n^2}{2E(S)} text{ with } E(S) = sum_{k=1}^{n-1} left( sum_{i=1}^{n-k}s_i cdot s_{i+k} right)^2$ where the sequence is of length n $S:= left(s_1, ldots,s_n right)$ with $s_i= pm 1$ . To obtain a pseudo-Boolean problem, we use the straightforward interpretation $s_i=2x_i-1$ for all $i in [n]$. F19-F21: The Ising Model The classical Ising model cite{Ising_Barahona1982} considers a set of spins placed on a regular lattice $G=([n],E)$, where each edge $(i,j) in {E}$ is associated with an interaction strength $J_{ij}$. Given a configuration of $n$ spins, $S:= left(s_1, ldots,s_n right)$, this problem poses a quadratic function, representing the system’s energy and depending on its structure $J_{ij}$. Assuming zero external magnetic fields and using $s_i=2x_i-1$ we obtain the following pseudo-Boolean maximization problem: $[ISING:] sum limits_{ {i,j} in {E}} left[x_{i}x_{j} - left(1-x_{i} right) left(1-x_{j} right) right] $ F19 considers the one-dimensional ring, F20 considers the two-dimensional torus, and F21 considers the two-dimensional triangular. F22: Maximum Independent Vertex Set Given a graph $G=([n],E)$ , an independent vertex set is a subset of vertices where no two vertices are linked by an edge. A maximum independent vertex set (MIVS) is defined as an independent subset Given a graph $V^{ prime} subset [n]$ having largest possible size. $[MIVS:] sum_i x_i ; textrm{s.t.} ; sum_{i &lt; j} x_i x_j e_{ij} = 0~$. F23: N-Queens Problem The N-queens problem (NQP) is defined as the task to place N queens on an ${N} times{N}$ chessboard in such a way that they cannot attack each other. Selection of upper problems is suggested in the paper https://doi.org/10.1145/3319619.3326810 .",
    "url": "http://localhost:4000/Benchmark/Problems/",
    "relUrl": "/Benchmark/Problems/"
  },
  "8": {
    "id": "8",
    "title": "Programming Interface",
    "content": "Programming Interface Data Structure and Manipulation Here, it is assumed that the data to be loaded follow exactly the aforementioned format regulation. A method DataSetList is provided to load the data: &gt; dsList &lt;- DataSetList(&#39;./data/RLS&#39;) Processing ./data/RLS/IOHprofiler_f1_i1.info ... algorithm RLS... 25 instances on f1 16D... 25 instances on f1 100D... ... The return value of method DataSetList is a S3 object, that is inherited from the list class. Consequently, dsList object can be sliced, indexed and printed as with lists: &gt; dsList DataSetList: 1: DataSet(RLS on f1 16D) 2: DataSet(RLS on f1 100D) 7: DataSet(RLS on f23 16D) 8: DataSet(RLS on f23 100D) &gt; dsList[1:3] DataSetList: 1: DataSet(RLS on f1 16D) 2: DataSet(RLS on f1 100D) 3: DataSet(RLS on f19 16D) &gt; dsList[[1]] DataSet(RLS on f1 16D) In addition, the summary method is implemented to show some basic information: &gt; summary(dsList) funcId DIM algId datafile comment 1 1 16 RLS ./data/RLS/data_f1/IOHprofiler_f1_DIM16_i1.dat % 2 1 100 RLS ./data/RLS/data_f1/IOHprofiler_f1_DIM100_i1.dat % 3 19 16 RLS ./data/RLS/data_f19/IOHprofiler_f19_DIM16_i1.dat % 4 19 100 RLS ./data/RLS/data_f19/IOHprofiler_f19_DIM100_i1.dat % 5 2 16 RLS ./data/RLS/data_f2/IOHprofiler_f2_DIM16_i1.dat % 6 2 100 RLS ./data/RLS/data_f2/IOHprofiler_f2_DIM100_i1.dat % 7 23 16 RLS ./data/RLS/data_f23/IOHprofiler_f23_DIM16_i1.dat % 8 23 100 RLS ./data/RLS/data_f23/IOHprofiler_f23_DIM100_i1.dat % Note that, column funcId stands for the numbering (ID) of test functions and algId is the identifier of the algorithm that is tested. Those columns (also DIM) are the attribute of dsList object, which are directly retrieved from the meta data (*.info files). Therefore, it is important to keep the meta data correct if it were prepared by the user manually. All the attributes DataSetList object are listed as follows: &gt; attributes(dsList) $class [1] &quot;list&quot; &quot;DataSetList&quot; $DIM [1] 16 100 16 100 16 100 16 100 $funcId [1] 1 1 19 19 2 2 23 23 $algId [1] &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; When subsetting (filtering) is needed for dsList, attributes DIM, funcId and algId can be used as follows: &gt; subset(dsList, DIM == 16, funcId == 1) DataSetList: 1: DataSet(RLS on f1 16D) &gt; subset(dsList, DIM == 16, algId != &#39;RLS&#39;) DataSetList: Now we could load the data files of the $(1, lambda)$-GA algorithm in the same way: &gt; dsList_ga &lt;- DataSetList(&#39;./data/self_GA&#39;, verbose = FALSE) Here, the argument verbose is set to FALSE to hide the prompting message. As with the R list, ``DataSetList’’ objects can be combined together: &gt; dsList &lt;- c(dsList, dsList_ga) Each element of dsList is a proglang{S3} object of type DataSet, which is again inherited from the list class. &gt; ds &lt;- dsList[[1]] &gt; ds DataSet(RLS on f1 16D) &gt; summary(ds) DataSet Object: Algorithm: RLS Function ID: 1 Dimension: 16D 25 instance found: 1,1,1,1,1,2,2,...,4,4,4,5,5,5,5,5 runtime summary: algId target mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% ERT runs ps 1: RLS 4 1.00 1 0.000000 1 1 1 1 1 1 1 1 1 1.00 25 1 2: RLS 5 1.04 1 0.200000 1 1 1 1 1 1 1 1 1 1.04 25 1 3: RLS 6 1.28 1 1.208305 1 1 1 1 1 1 1 2 2 1.28 25 1 11: RLS 14 21.00 21 9.165151 5 5 7 14 18 26 34 37 37 21.00 25 1 12: RLS 15 29.16 29 10.466932 12 12 15 18 26 36 40 48 48 29.16 25 1 13: RLS 16 46.48 48 21.652790 13 13 19 26 42 58 71 83 83 46.48 25 1 function value summary: algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: RLS 1 25 7.68 7 1.7729448 4.48 5.2 6.0 7 7 9 10.0 10.8 11.00 2: RLS 2 25 8.28 8 1.6206994 5.48 6.0 6.4 7 8 9 10.0 10.8 11.52 3: RLS 3 25 8.76 9 1.5885003 5.48 6.2 7.0 8 9 10 10.6 11.0 11.52 64: RLS 2511 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 65: RLS 2818 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 66: RLS 3162 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 Attributes: names, class, suite, funcId, DIM, algId, algInfo, comment, datafile, instance, maxRT, finalFV, format, maximization In the summary method, the data set is summarized in two perspectives: Fixed-target perspective: the method looks for the first hitting time, that is the number of function evaluations an algorithm takes to reach a target function value (target above) for the first time. The target values are automatically determined and evenly spaced in the observed range. Some basic statistics on the running time sample are calculated for each target value: mean, median, standard deviation (sd), percentiles (2% 5% 10% …), the expected running time (ERT) and success rate (ps, the ratio of successful runs our of all the independent runs). Fixed-budget perspective: the method looks for the best function value reached by the algorithm, when a specific number of function evaluations (budget) are taken (runtime above). The budget values are automatically determined and evenly spaced in the observed range. Roughly the same set of statistics are provided as the fixed-target perspective. In the ds object, two matrices are always stored for those two perspectives explained above: ds$RT: running time samples in the fixed-target perspective and ds$FV: function value samples in the fixed-budget perspective. Note that, when the parameter tracking is enabled, the parameter of interest is also arranged in the fixed-target perspective and is appended to ds object. For instance, if mutation_rate is the parameter name given to the benchmark, the parameter can be obtained by ds$mutation_rate. We could take a glimpse at those two matrices: &gt; head(ds$RT) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 2 1 1 6 1 1 1 1 1 1 1 1 2 1 1 7 1 1 7 1 1 1 1 1 3 1 1 5 1 1 8 1 1 8 1 3 2 2 1 4 1 1 7 2 1 10 2 1 9 1 4 3 3 4 5 1 1 8 4 2 11 3 1 Here, the column names are the target values and in the example below, those are the budget values: &gt; head(ds$FV) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] 1 11 7 7 7 8 6 9 9 5 7 8 4 7 11 2 11 7 8 8 8 6 9 10 6 8 9 5 8 12 3 11 8 9 9 8 7 10 10 6 8 9 5 9 12 4 12 9 10 9 9 8 10 11 6 9 10 5 9 13 5 12 9 10 9 10 9 10 12 7 10 10 5 10 14 6 13 9 11 10 10 10 10 13 7 11 11 5 10 14 Retrieving Performance Data For both DataSet and DataSetList objects, the overview of the observed running time/function value can obtained using: &gt; get_RT_overview(subset(dsList, algId == &#39;RLS&#39;)) Algorithm DIM fID miminal runtime maximal runtime runs Budget 1: RLS 16 1 1 96 25 16000 2: RLS 100 1 1 1128 25 100000 3: RLS 16 19 1 52 25 16000 4: RLS 100 19 1 570 25 100000 5: RLS 16 2 1 255 25 16000 6: RLS 100 2 1 6546 25 100000 7: RLS 16 23 1 67 25 16000 8: RLS 100 23 1 641 25 100000 &gt; get_RT_overview(ds) algId DIM funcId miminal runtime maximal runtime runs Budget 1: RLS 16 1 1 96 25 16000 Here, Budget indicates the maximal allowable budget that is given when running the experiment while maximal runtime is the maximal observed running time in each triplet of (algId, DIM, funcId). For the function values, the similar methods are also implemented: &gt; get_FV_overview(subset(dsList, algId == &#39;RLS&#39;)) algId DIM funcId worst recorded worst reached best reached mean reached median reached runs succ budget 1: RLS 16 1 4 16 16 16.00 16 25 25 16000 2: RLS 100 1 44 100 100 100.00 100 25 25 100000 3: RLS 16 19 8 20 32 25.60 24 25 1 16000 4: RLS 100 19 92 152 172 162.56 164 25 3 100000 5: RLS 16 2 0 16 16 16.00 16 25 25 16000 6: RLS 100 2 0 100 100 100.00 100 25 25 100000 7: RLS 16 23 -100 3 4 3.24 3 25 6 16000 8: RLS 100 23 -1868 7 9 8.20 8 25 9 100000 It is important to distinguish some columns in the example here: worst recorded stands for the worst (smallest) function value observed in all independent runs for each case of (algId, DIM, funcId). In contrast, worst reached means the smallest value reached in the last iteration (across independent runs) of the algorithm while best reached the largest value. runs gives the total number of independent runs in each case while succ is the number of runs where the corresponding best reached is hit. Note that, in our naming convention of methods, RT is always the abbreviation of running time and FV is for function value (the same below). To get a data summary at arbitrary running time/function value, two methods, get_RT_summary and get_FV_summary are implemented. Let use the object ds (defined before) to illustrate the usage: &gt; ds DataSet(RLS on f1 16D) &gt; get_RT_summary(ds, ftarget = c(5, 10, 16)) algId target mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% ERT runs ps 1: RLS 5 1.04 1 0.200000 1 1 1 1 1 1 1 1 1 1.04 25 1 2: RLS 10 5.24 5 3.455431 1 1 1 2 5 6 10 12 12 5.24 25 1 3: RLS 16 46.48 48 21.652790 13 13 19 26 42 58 71 83 83 46.48 25 1 &gt; get_FV_summary(ds, runtime = c(10, 50, 100)) algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: RLS 10 25 11.52 12 1.5307950 8.48 9 9.4 11 12 13 13 13.8 14 2: RLS 50 25 15.64 16 0.4898979 15.00 15 15.0 15 16 16 16 16.0 16 3: RLS 100 25 16.00 16 0.0000000 16.00 16 16.0 16 16 16 16 16.0 16 The input for the argument ftarget and runtime should be provided by the user. In this example, three values are chosen arbitrarily in the corresponding range of running time/function value (cf. the first lines of get_RT_overview and get_FV_overview above). Furthermore, using the magrittr package (please install it if necessary), it is possible to ``chain’’ all the methods introduced so far, making the code snippet more readable: &gt; library(magrittr) &gt; dsList %&gt;% + subset(DIM == 100, algId == &#39;RLS&#39;, funcId == 19) %&gt;% + get_FV_summary(runtime = seq(1, 5000, length.out = 5)) DIM funcId algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: 100 19 RLS 1.00 25 104.16 104 8.284926 92 92.0 93.6 100 104 108 116.8 120 120 2: 100 19 RLS 1250.75 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 3: 100 19 RLS 2500.50 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 4: 100 19 RLS 3750.25 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 5: 100 19 RLS 5000.00 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 In addition, it is also straightforward to retrieve the raw sample points of the running time/function value, using the following methods, get_RT_sample and get_FV_sample: &gt; get_RT_sample(ds, ftarget = 10, output = &#39;long&#39;) algId target run RT 1: RLS 10 1 1 2: RLS 10 2 8 3: RLS 10 3 4 23: RLS 10 23 12 24: RLS 10 24 1 25: RLS 10 25 8 begin{CodeChunk} small begin{CodeInput} get_FV_sample(ds, runtime = c(5, 20), output = ‘wide’) algId runtime run.1 run.2 run.3 run.4 run.5 run.6 run.7 run.8 run.9 run.10 run.11 run.12 1: RLS 5 12 9 10 9 10 9 10 12 7 10 10 5 2: RLS 20 16 12 15 14 14 13 14 15 14 12 14 13 Note that, all methods in this sub-section return a data.table object from the data.table package. Plotting To visualize the benchmark data, a collection of plotting methods are implemented in iohana. In this section, we shall provide examples on some important plots using the data set in the last section. Here, only the plotting method for the fixed-target perspective is shown because the same set of methods are implemented for the fixed-budget perspective. Firstly, the progression of the function value is plotted against the running time: &gt; ds_plot &lt;- subset(dsList, DIM == 16, funcId == 1) &gt; Plot.RT.Single_Func(ds_plot) The data sets on $16D$, function $F1$ are plotted here, which is shown in the following figure. Note that, a interactive plot is created as the plotly library is used here by default. The static plotting library *ggplot2 can also be selected by setting argument backend = ‘ggplot2’ (this is only a difference in the plotting backend and thus it will not be demonstrated here). In the figure, the standard deviation of the running time is also drawn. &gt; ?Plot.RT.Single_Func In addition, the previous plot can be grouped by functions, using Plot.RT.Multi_Func methods. The example is shown below. &gt; Plot.RT.Multi_Func(ds_plot, scale.ylog = T) Given a target value, Plot.RT.Histogram methods renders the histogram of the running time required to reach this target value. Taking the data set on F23 and 16D as an example, it is important to view the range of function values through method get_FV_overview as shown in the previous section: &gt; ds &lt;- subset(dsList, DIM == 16, funcId == 23) &gt; get_FV_overview(ds) algId DIM funcId worst recorded worst reached best reached ... Budget 1: RLS 16 23 -100 3 4 ... 16000 2: self_GA 16 23 -96 4 4 ... 16001 Then, we choose a target value $-3$ that is close to the best reached value and plot the histogram. &gt; Plot.RT.Histogram(ds, ftarget = -3, plot_mode = &#39;subplot&#39;) The argument plot_mode = ‘subplot’ will create a separate sub-plot for each algorithm in the data set. In addition, the empirical density function of the running time, that is estimated by the Kernel Density Estimation (KDE) method, can be generated by method Plot.RT.PMF. &gt; Plot.RT.PMF(ds, ftarget = -3, show.sample = TRUE) Finally, it is also crucial to look at the Empirical Cumulative Distribution function (ECDF) of the running time. For this purpose, three methods are implemented for different levels of data aggregation: Plot.RT.ECDF_Per_Target: it only compares the ECDF of algorithms on a single target value, e.g., &gt; Plot.RT.ECDF_Per_Target(ds, ftarget = -1) Plot.RT.ECDF_Single_Func: it takes as input an array of target values (controlled by arguments fstart, fstop, fstep) and aggregates the ECDF over those targets, e.g., &gt; Plot.RT.ECDF_Single_Func(ds, fstart = -92, fstop = 4, fstep = 10) Plot.RT.ECDF_Multi_Func: it, in addition, aggregates different target values over all test function in a data set. To demonstrate its usage, let’s take the data set on 100D and check the overview of the function values. Then three target values are chosen manually for each function, which are collected in a list object. The resulting plot is shown i. r ds &lt;- subset(dsList, DIM == 100) get_FV_overview(ds) algId DIM funcId worst recorded worst reached best reached … budget 1: RLS 100 1 44 100 100 … 100000 2: RLS 100 19 92 152 172 … 100000 3: RLS 100 2 0 100 100 … 100000 4: RLS 100 23 -1868 7 9 … 100000 5: self_GA 100 1 38 98 100 … 100001 6: self_GA 100 19 72 164 192 … 100001 7: self_GA 100 2 0 39 100 … 100001 8: self_GA 100 23 -1761 7 10 … 100001 ftarget &lt;- list(1 = c(80, 90, 100), 2 = c(80, 90, 100), 19 = c(180, 190, 200), 23 = c(0, 5, 10)) Plot.RT.ECDF_Multi_Func(ds, ftarget) //: #",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/ProgrammingInterface/",
    "relUrl": "/IOHanalyzer(Post-Processing)/ProgrammingInterface/"
  },
  "9": {
    "id": "9",
    "title": "Transformation",
    "content": "Transformation Instead of testing one particular problem $f$ only, the user can choose to run experiments on several problem instances that are obtained from $f$ through a set of transformations. In its most general form,IOHprofiler currently offers to return to the algorithm the values as $af( sigma (x { oplus} z)) + b$, where $a$ is a multiplicative shift of the function value, $b$ is an additive shift of the function value, ${ oplus}z: {0,1}^n to {0,1}^n, (x_1, … ,x_n) mapsto ((x_1 + z_1) {mod} 2, … ,(x_n + z_n) {mod} 2)$ is an XOR-shift of the search point, $ mathcal{S}^n to mathcal{S}^n, (x_1, … ,x_n) mapsto (x_{ sigma(1)}, … ,x_{ sigma(n)})$ is a permutation of the search point. Note here that, in abuse of notation, we identify the permutation ${ sigma}: [1..n] to [1..n]$ with the here-defined re-ordering of the bit string. In practical, we applied $af(x { oplus} z) + b$ to instance 2-50, and $af( sigma (x)) + b$ to instance 51-100. $a in [ frac{1}{5},5]$ and $b in [-1000,1000]$ are randomly generated.",
    "url": "http://localhost:4000/Benchmark/Transformation/",
    "relUrl": "/Benchmark/Transformation/"
  },
  "10": {
    "id": "10",
    "title": "About",
    "content": "IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics IOHprofiler is a new tool for analyzing and comparing iterative optimization heuristics. Given as input algorithms and problems written in C, C++, or Python*, it provides as output an in-depth statistical evaluation of the algorithms’ fixed-target and fixed-budget running time distributions. In addition to these performance evaluations, IOHprofiler also allows to track the evolution of algorithm parameters, making our tool particularly useful for the analysis, comparison, and design of (self-)adaptive algorithms. IOHprofiler is a ready-to-use software. It consists of two parts: IOHExperimenter, which generates the running time data; and IOHAnalyzer, which produces the summarizing comparisons and statistical evaluations. Currently IOHExperimenter is built on the COCO software, but a new C++ based version is developing and will be released soon. [This code] implements the experimentation tool of IOHprofiler. For the analyzer part, please visit IOHAnalyzer page. Documentation IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics Contact If you have any questions, comments, suggestions or pull requests, please don’t hesitate contacting us IOHprofiler@liacs.leidenuniv.nl! Cite us",
    "url": "http://localhost:4000/about/",
    "relUrl": "/about/"
  },
  "11": {
    "id": "11",
    "title": "",
    "content": "News Welcome to IOHprofiler IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics IOHprofiler is a new tool for analyzing and comparing iterative optimization heuristics. Given as input algorithms and problems written in C, C++, or Python*, it provides as output an in-depth statistical evaluation of the algorithms’ fixed-target and fixed-budget running time distributions. In addition to these performance evaluations, IOHprofiler also allows to track the evolution of algorithm parameters, making our tool particularly useful for the analysis, comparison, and design of (self-)adaptive algorithms. IOHprofiler is a ready-to-use software. It consists of two parts: IOHExperimenter, which generates the running time data; and IOHAnalyzer, which produces the summarizing comparisons and statistical evaluations. Currently IOHExperimenter is built on the COCO software, but a new C++ based version is developing and will be released soon. [This code] implements the experimentation tool of IOHprofiler. For the analyzer part, please visit IOHAnalyzer page.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "12": {
    "id": "12",
    "title": "News",
    "content": "News Welcome to IOHprofiler",
    "url": "http://localhost:4000/News/",
    "relUrl": "/News/"
  }
  
}
