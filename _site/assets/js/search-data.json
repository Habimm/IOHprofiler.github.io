{
  "0": {
    "id": "0",
    "title": "Benchmark",
    "content": "Benchmark PBO_suite : 23 pseudo-Boolean Problems.",
    "url": "http://localhost:4000/Benchmark/",
    "relUrl": "/Benchmark/"
  },
  "1": {
    "id": "1",
    "title": "CSVLogger",
    "content": "IOHprofiler_csv_logger IOHprofiler_csv_logger provides methods storing output into csv files. For each problems, .info is created to store best found fitness and the first evaluation time that the best fitness has been found for each run. In addition, some data files are generated in sub-folder of each problem to store details of optimization process. The type of generated files is decided by the strategies of IOHprofiler_observer being used. The relationship between these strategies and files are as follow. .dat files store evaluation information with target-based tracking strategy .cdat files store evaluation information with complete tracking strategy .idat files store evaluation information with interval tracking strategy .tdat files store evaluation information with time-based tracking_ strategy For the complete list of setting parameters of IOHprofiler_csv_logger, please visit the github page.",
    "url": "http://localhost:4000/IOHexperimenter/Loggers/CSVLogger/",
    "relUrl": "/IOHexperimenter/Loggers/CSVLogger/"
  },
  "2": {
    "id": "2",
    "title": "CreatingProblems",
    "content": "Creating Problems IOHprofiler_problem is the base class of problems of IOHexperimenter. The property variables of problems include: problem_id, will be assigned if the problem is added to a suite, otherwise default by 0. instance_id, sets transformation methods on problems. The original problem is with instance_id 1, scale and shift are applied on objectives for instance_id in [2,100], XOR is applied on variables for instance_id in [2,50], and sigma function is applied on variables for instance_id in [51,100]. problem_name problem_type lowerbound, is a vector of lowerbound for variables. upperbound, is a vector of upperbound for variables. number_of_variables, is the dimension of the problem. number_of_objectives, is only available as 1 now. The functionality of multi-objectives is under development. best_variables, is a vector of optimal solution, which is used to calculate the optimum. If the best_variables is not given, the optimum will be set as DBL_MAX. optimal, is a vector of optimal objectives, but currently only single objective is supported. evaluate_int_info, is a vector of int values that are iteratively used in evaluate. evaluate_double_info, is a vector of double values that are iteratively used in evaluate. And some functions for personal experiments are supplied: evaluate(x), returns a vector of fitness values. The argument x is a vector of variables. evaluate(x,y), updates y with a vector of fitness values, and x is a vector of variables. addCSVLogger(logger), assigns a IOHprofiler_csv_logger class to the problem. clearLogger(), delete logger methods of the problem. reset_problem(), reset the history information of problem evaluations. You should call this function at first when you plan to do another test on the same problem class. IOHprofiler_hit_optimal(), returns true if the optimum of the problem has been found. IOHprofiler_set_number_of_variables(number_of_variables), sets dimension of the problem. IOHprofiler_set_instance_id(instance_id) IOHexperimenter provides a variety of problems for testing algorithms, but it is also easy to add your own problems. Overall, to create a problem of IOHexperimenter, two functions need to be implemented: construct functions and internel_evaluate. Additionally, you can define update_evaluate_double_info and update_evaluate_int_info to make evluate process more efficiently. Taking the implementation of OneMax as an instance, construct functions are as below. problem_name and number_of_objectives must be set. In general, two methods of construction of the problems are given. One is constructing without giving instance_id and dimension, and the other one is with. OneMax() { IOHprofiler_set_problem_name(&quot;OneMax&quot;); IOHprofiler_set_problem_type(&quot;pseudo_Boolean_problem&quot;); IOHprofiler_set_number_of_objectives(1); IOHprofiler_set_lowerbound(0); IOHprofiler_set_upperbound(1); IOHprofiler_set_best_variables(1); } OneMax(int instance_id, int dimension) { IOHprofiler_set_instance_id(instance_id); IOHprofiler_set_problem_name(&quot;OneMax&quot;); IOHprofiler_set_problem_type(&quot;pseudo_Boolean_problem&quot;); IOHprofiler_set_number_of_objectives(1); IOHprofiler_set_lowerbound(0); IOHprofiler_set_upperbound(1); IOHprofiler_set_best_variables(1); Initilize_problem(dimension); } ~OneMax() {}; void Initilize_problem(int dimension) { IOHprofiler_set_number_of_variables(dimension); IOHprofiler_set_optimal((double)dimension); }; The internal_evaluate must be implemented as well. It is used during evaluate process, returning a vector of (real) objective values of the corresponding variables x. std::vector&lt;double&gt; internal_evaluate(std::vector&lt;int&gt; x) { std::vector&lt;double&gt; y; int n = x.size(); int result = 0; for (int i = 0; i != n; ++i) { result += x[i]; } y.push_back((double)result); return y; }; If you want to register your problem by problem_name and add it into a suite, please add functions creating instance as following codes. static OneMax * createInstance() { return new OneMax(); }; static OneMax * createInstance(int instance_id, int dimension) { return new OneMax(instance_id, dimension); }; To register the problem, you can use the geniricGenerator in IOHprofiler_class_generator. For example, you can use the following statement to register and create OneMax , // Register static registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax&gt; regOneMax(&quot;OneMax&quot;); // Create std::shared_ptr&lt;IOHprofiler_problem&lt;int&gt;&gt; problem = genericGenerator&lt;IOHprofiler_problem&lt;int&gt;&gt;::instance().create(&quot;OneMax&quot;);",
    "url": "http://localhost:4000/IOHexperimenter/CreatingProblems/",
    "relUrl": "/IOHexperimenter/CreatingProblems/"
  },
  "3": {
    "id": "3",
    "title": "CreatingSuites",
    "content": "Creating Suites IOHprofiler_suite is the base class of suites of IOHexperimenter. The property variables of problems include: problem_id, a vector containing the ids of the problems to be tested. instance_id, a vector containing the ids of the instances of the problems. Intance ids specify which transformations will be applied to the problem. The original problem has instance_id 1; scale and shift are applied on objectives for instance_id in [2,100]; XOR will be applied on variables for instance_id in [2,50], and sigma function is applied on variables for instance_id in [51,100]. dimension, a vector containing the dimensions of the problems. number_of_problems number_of_instances number_of_dimensions The following functions for experiments are available to a suite: get_next_problem, return a shared point of problems of the suite in order. addCSVLogger(logger), assigns a IOHprofiler_csv_logger class to the suite. IOHprofiler_set_suite_problem_id(problem_id) IOHprofiler_set_suite_instance_id(instance_id) IOHprofiler_set_suite_dimension(dimension) mapIDTOName, is to match problem id and name. IOHexperimenter provides a PBO_suite for pseudo Boolean problems, but it is also easy to add your own suite. Creating a suite is done by registering problems in the suite and assigning ids to them. Taking the implementation of PBO_suite as an example, constructor functions are as below. In the constructor functions, the range of allowed problem_id, instance_id and dimension should be identified. In addition, registerProblem() must be included in the constructor functions. PBO_suite() { std::vector&lt;int&gt; problem_id; std::vector&lt;int&gt; instance_id; std::vector&lt;int&gt; dimension; for (int i = 0; i &lt; 23; ++i) { problem_id.push_back(i+1); } for (int i = 0; i &lt; 1; ++i) { instance_id.push_back(i+1); } dimension.push_back(100); IOHprofiler_set_suite_problem_id(problem_id); IOHprofiler_set_suite_instance_id(instance_id); IOHprofiler_set_suite_dimension(dimension); IOHprofiler_set_suite_name(&quot;PBO&quot;); registerProblem(); }; PBO_suite(std::vector&lt;int&gt; problem_id, std::vector&lt;int&gt; instance_id, std::vector&lt;int&gt; dimension) { for (int i = 0; i &lt; problem_id.size(); ++i) { if (problem_id[i] &lt; 0 || problem_id[i] &gt; 23) { IOH_error(&quot;problem_id &quot; + std::to_string(problem_id[i]) + &quot; is not in PBO_suite&quot;); } } for (int i = 0; i &lt; instance_id.size(); ++i) { if (instance_id[i] &lt; 0 || instance_id[i] &gt; 100) { IOH_error(&quot;instance_id &quot; + std::to_string(instance_id[i]) + &quot; is not in PBO_suite&quot;); } } for (int i = 0; i &lt; dimension.size(); ++i) { if (dimension[i] &lt; 0 || dimension[i] &gt; 20000) { IOH_error(&quot;dimension &quot; + std::to_string(dimension[i]) + &quot; is not in PBO_suite&quot;); } } IOHprofiler_set_suite_problem_id(problem_id); IOHprofiler_set_suite_instance_id(instance_id); IOHprofiler_set_suite_dimension(dimension); IOHprofiler_set_suite_name(&quot;PBO&quot;); registerProblem(); } registerProblem() is a virtual function of the base IOHprofiler_suite class. When you create a suite, it must be implemented. Problems to be included in the suite can be registered by name. Afterwards, problem id and name should be mapped through mapIDTOName&gt; function, which enables the suite to recognize problems by problem id. Following is the registerProblem() function of PBO_suite. registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax&gt; regOneMax(&quot;OneMax&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Dummy1&gt; regOneMax_Dummy1(&quot;OneMax_Dummy1&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Dummy2&gt; regOneMax_Dummy2(&quot;OneMax_Dummy2&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Epistasis&gt; regOneMax_Epistasis(&quot;OneMax_Epistasis&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Neutrality&gt; regOneMax_Neutrality(&quot;OneMax_Neutrality&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Ruggedness1&gt; regOneMax_Ruggedness1(&quot;OneMax_Ruggedness1&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Ruggedness2&gt; regOneMax_Ruggedness2(&quot;OneMax_Ruggedness2&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,OneMax_Ruggedness3&gt; regOneMax_Ruggedness3(&quot;OneMax_Ruggedness3&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes&gt; regLeadingOnes(&quot;LeadingOnes&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Dummy1&gt; regLeadingOnes_Dummy1(&quot;LeadingOnes_Dummy1&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Dummy2&gt; regLeadingOnes_Dummy2(&quot;LeadingOnes_Dummy2&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Epistasis&gt; regLeadingOnes_Epistasis(&quot;LeadingOnes_Epistasis&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Neutrality&gt; regLeadingOnes_Neutrality(&quot;LeadingOnes_Neutrality&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Ruggedness1&gt; regLeadingOnes_Ruggedness1(&quot;LeadingOnes_Ruggedness1&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Ruggedness2&gt; regLeadingOnes_Ruggedness2(&quot;LeadingOnes_Ruggedness2&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LeadingOnes_Ruggedness3&gt; regLeadingOnes_Ruggedness3(&quot;LeadingOnes_Ruggedness3&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,Linear&gt; regLinear(&quot;Linear&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,MIS&gt; regMIS(&quot;MIS&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,LABS&gt; regLABS(&quot;LABS&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,NQueens&gt; regNQueens(&quot;NQueens&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,Ising_1D&gt; regIsing_1D(&quot;Ising_1D&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,Ising_2D&gt; regIsing_2D(&quot;Ising_2D&quot;); registerInFactory&lt;IOHprofiler_problem&lt;int&gt;,Ising_Triangle&gt; regIsing_Triangle(&quot;Ising_Triangle&quot;); mapIDTOName(1,&quot;OneMax&quot;); mapIDTOName(2,&quot;LeadingOnes&quot;); mapIDTOName(3,&quot;Linear&quot;); mapIDTOName(4,&quot;OneMax_Dummy1&quot;); mapIDTOName(5,&quot;OneMax_Dummy2&quot;); mapIDTOName(6,&quot;OneMax_Neutrality&quot;); mapIDTOName(7,&quot;OneMax_Epistasis&quot;); mapIDTOName(8,&quot;OneMax_Ruggedness1&quot;); mapIDTOName(9,&quot;OneMax_Ruggedness2&quot;); mapIDTOName(10,&quot;OneMax_Ruggedness3&quot;); mapIDTOName(11,&quot;LeadingOnes_Dummy1&quot;); mapIDTOName(12,&quot;LeadingOnes_Dummy2&quot;); mapIDTOName(13,&quot;LeadingOnes_Neutrality&quot;); mapIDTOName(14,&quot;LeadingOnes_Epistasis&quot;); mapIDTOName(15,&quot;LeadingOnes_Ruggedness1&quot;); mapIDTOName(16,&quot;LeadingOnes_Ruggedness2&quot;); mapIDTOName(17,&quot;LeadingOnes_Ruggedness3&quot;); mapIDTOName(18,&quot;LABS&quot;); mapIDTOName(22,&quot;MIS&quot;); mapIDTOName(19,&quot;Ising_1D&quot;); mapIDTOName(20,&quot;Ising_2D&quot;); mapIDTOName(21,&quot;Ising_Triangle&quot;); mapIDTOName(23,&quot;NQueens&quot;); If you want to register your suite called suite_name, please add following codes and modify names. static PBO_suite * createInstance() { return new PBO_suite(); }; static PBO_suite * createInstance(std::vector&lt;int&gt; problem_id, std::vector&lt;int&gt; instance_id, std::vector&lt;int&gt; dimension) { return new PBO_suite(problem_id, instance_id, dimension); }; To register the suite, you can use the geniricGenerator in IOHprofiler_class_generator. For example, you can use the following statement to register and create PBO_suite , // Register static registerInFactory&lt;IOHprofiler_suite&lt;int&gt;,PBO_suite&gt; regPBO(&quot;PBO&quot;); // Create std::shared_ptr&lt;IOHprofiler_suite&lt;InputType&gt;&gt; suite = genericGenerator&lt;IOHprofiler_suite&lt;int&gt;&gt;::instance().create(&quot;PBO&quot;); );",
    "url": "http://localhost:4000/IOHexperimenter/CreatingSuites/",
    "relUrl": "/IOHexperimenter/CreatingSuites/"
  },
  "4": {
    "id": "4",
    "title": "Experiments",
    "content": "Getting Started Getting started by C++ Getting started by R Using IOHexperimenter by C++ After compiling the tool by executing make in the root directory, /bin and /obj subfolders are to be created in this folder. To use IOHexperimenter to test your algorithms, you can work this C folder and replace the Makefile by Makefile-local. Afterwards, you can edit your algorithm in the provided cpp files and compile them by using the make statement. There are three ways to test algorithms: Test using individual problems Test using collections of problems (suites) Test using an experiment with a configuration file Test using individual problems All problems of IOHexperimenter are defined as specific derived class inheriting problem IOHprofiler_problem class, the source codes are available in the Problems folder. For the definition of the problems already implemented in IOHexperimenter, please visit the wiki page https://iohprofiler.github.io/Benchmark/Problems/. An example testing evolutionary algorithm with mutation operator on OneMax is implemented in IOHprofiler_run_problem.cpp. For this example, a OneMax class is declared and initialized with dimension 1000 on the default instance of the probelem. OneMax om; int dimension = 1000; om.Initilize_problem(dimension); During the optimization process, the algorithm can acquire the fitness value through evaluate() function. In the example below, om.evaluate(x) returns the fitness of x. Another option is the statement om.evaluate(x,y), which stores the fitness of x in y. In addition, om.IOHprofiler_hit_optimal() is an indicator you can use to check if the optimum has been found. while(!om.IOHprofiler_hit_optimal()) { copyVector(x_star,x); if(mutation(x,mutation_rate)) { y = om.evaluate(x); } if(y[0] &gt; best_value) { best_value = y[0]; copyVector(x,x_star); } } If, for your experiment, you want to generate data to be used in the IOHanalyzer, a IOHprofiler_csv_logger should be added to the problem you are testing on. The arguments of IOHprofiler_csv_logger are directory of result folder, name of result folder, name of the algorithm and infomation of the algorithm. With different setting of triggers (observer), mutilple data files are to be generated for each experiment. For more details on the available triggers, please visit the introduction of IOHprofiler_observer. std::vector&lt;int&gt; time_points{1,2,5}; std::shared_ptr&lt;IOHprofiler_csv_logger&gt; logger(new IOHprofiler_csv_logger(&quot;./&quot;,&quot;run_problem&quot;,&quot;EA&quot;,&quot;EA&quot;)); logger-&gt;set_complete_flag(true); logger-&gt;set_interval(0); logger-&gt;set_time_points(time_points,10); logger-&gt;activate_logger(); om.addCSVLogger(std::move(logger)); Test using suites Suites are collections of test problems. The idea behind a suite is that packing problems with similar properties toghther makes it easier to test algorithms on a class of problems. Currently a suite called PBO consisting of 23 pseudo Boolean problems is provied by IOHexperimenter. To find out how to create your own suites, please visit this page. An example of testing an evolutionary algorithm with mutation operator on the PBO suite is implemented in IOHprofiler_run_suite.cpp. PBO suite includes pointers to 23 problems. To instantiate problems you want to test, the vectors of problem id, instances and dimensions need to be given as follows: std::vector&lt;int&gt; problem_id = {1,2}; std::vector&lt;int&gt; instance_id ={1,2}; std::vector&lt;int&gt; dimension = {100,200,300}; PBO_suite pbo(problem_id,instance_id,dimension); With the suite, you can test problems of the suite one by one, until all problems have been tested. In this example, the order of problem is as follow, and an evlutionary_algorithm is applied: problem id 1, instance 1, dimension 100 problem id 1, instance 2, dimension 100 problem id 1, instance 1, dimension 200 problem id 1, instance 2, dimension 200 problem id 1, instance 1, dimension 300 problem id 1, instance 2, dimension 300 problem id 2, instance 1, dimension 100 problem id 2, instance 2, dimension 100 problem id 2, instance 1, dimension 200 problem id 2, instance 2, dimension 200 problem id 2, instance 1, dimension 300 problem id 2, instance 2, dimension 300 while (problem = pbo.get_next_problem()) { evolutionary_algorithm(problem); } If, for your experiment, you want to generate data to be used in the IOHanalyzer, a IOHprofiler_csv_logger should be added to the suite. The arguments of IOHprofiler_csv_logger are the directory of result folder, name of result folder, name of the algorithm and infomation of the algorithm. In addition, you can set up mutilple triggers of recording evaluations. For the details of triggers, please visit the introduction of IOHprofiler_observer. std::vector&lt;int&gt; time_points{1,2,5}; std::shared_ptr&lt;IOHprofiler_csv_logger&gt; logger(new IOHprofiler_csv_logger(&quot;./&quot;,&quot;run_suite&quot;,&quot;EA&quot;,&quot;EA&quot;)); logger-&gt;set_complete_flag(true); logger-&gt;set_interval(2); logger-&gt;set_time_points(time_points,3); logger-&gt;activate_logger(); pbo.addCSVLogger(logger); Test using an experiment with a configuration file By using the provided IOHprofiler_experiment class, you can use a configuration file to configure both the suite and the logger for csv files. To use the provided experiment structure, you need to provide both the path to the configuration file and the pointer to your optimization algorithm to the experimenter._run() function, which will execute all tasks of the experiment. In addition, you can set the number of repetitions for all problems by using experimenter._set_independent_runs(2). std::string configName = &quot;./configuration.ini&quot;; IOHprofiler_experimenter&lt;int&gt; experimenter(configName,evolutionary_algorithm); experimenter._set_independent_runs(2); experimenter._run(); For the content of configuration file, it consists of three sections: suite configures the problems to be tested. suite_name, is the name of the suite to be used. Please make sure that the suite with the corresponding name is registered. problem_id, configures problems to be tested. Note that id of problems are configured by the suite, please make sure that id is within the valid range. instance_id, configures the transformation methods applied on problems. For PBO: 1 : no transformer operations on the problem. 2-50 : XOR and SHIFT operations are applied on the problem. 51-100: SIGMA and SHIFT operations are applied on the problem. dimension, configures dimension of problems to be tested. Note that allowed dimension is not larger than 20000. logger configures the setting of output csv files. result_foler is the directory of the folder where sotres output files. algorithm_name, is the name of the algorithm, which is used when generating ‘.info’ files. algorithm_info, is additional information of the algorithm, which is used when generating ‘.info’ files. observer configures parameters of IOHprofiler_server, which is used in IOHprofiler_csv_logger, complete_triggers is the switch of .cdat files, which works with complete tracking strategy. Set it as TRUE or true if you want to output .cdat files. update_triggers is the switch of .dat files, which works with target-based strategy strategy. Set it as TRUE or true if you want to output .dat` files. number_interval_triggers configures the .idat files, which works with interval tracking number_target_triggers sets the value of the frequecny. If you do not want to generate .idat files, set number_target_triggers as 0. number_target_triggers configures the .tdat files, which works with time-based tracking strategy. base_evaluation_triggers configures the .tdat files, which works with time-based tracking strategy. To switch off .tdat files, set both number_target_triggers and base_evaluation_triggers as 0. Using IOHexperimenter by R For the use of R, please visit R branch.",
    "url": "http://localhost:4000/IOHexperimenter/Experiments/",
    "relUrl": "/IOHexperimenter/Experiments/"
  },
  "5": {
    "id": "5",
    "title": "Graphic User Interface",
    "content": "Graphic User Interface Data loading Fixed-target results Expected runtime Probability mass function Cumulative distribution A web-based GUI is implemented in IOHanalyzer such that the user can analyze the experiment data easily in the web browser using two R packages: shiny and plotly. In this section, we shall describe this Graphical User Interface (GUI) in detail. Moreover, for the users who are not familiar with R , a free GUI server is hosted online (For any bug report, feedback or requested change, the users are encouraged to contact us through iohprofiler@liacs.leidenuniv.nl). Alternatively, invoking the GUI is also straightforward on your local machine, given package IOHanalyzer is already installed: &gt; runServer() Loading required package: shiny Listening on http://127.0.0.1:3943 which will start the GUI server on the local machine (hence using IP address 127.0.0.1 and a random port number). The web browser will be launched and connect to this address immediately after starting the server. There are tour groups of functionalities on the page. Upload Data: This section provides functionality to upload experiment results or load an official data set that is provided by the author. Fixed-Target Results This section provides statistics covering the fixed-target perspective of performance evaluation. That is, the results in this section mainly address the question about the statistical property of running time (i.e., function evaluations) that is needed to obtain a solution of a desired target quality. Fixed-Budget Results This sectioncovers the fixed-budget perspective, that is the statistics on objective function values obtained by the search points a given budget of function evaluations. In other words, the results in this section mainly address the question how good the search points are that a user can expect to see within a given frame of running time. Algorithm Parameters This section provides details about the evolution of the algorithm parameters that the user specified to be tracked in the experimental procedure. Data loading The GUI interface to load the experiment data is shown in the following figure, in which the user is asked to upload a compressed archive. The following compression format are supported: *.zip, *.bz, *.tar, *.xz, *.gz. Note that, when the user’s data set is enormous to handle, it is possible to speed up the uploading (and hence plotting) procedure by toggling option Efficient mode on, in which a subset is taken from the huge data set. Moreover, when using the online GUI, the user can also load official data sets provided by the author, using the Load Data from Repository box on the right of the page. At the time of writing, two official data sets are made available, each of which contains results of 11 algorithms on all 23 test functions, over dimensions {16, 100, 625}. For the specification of those two data sets and updates on the data set, the user is suggested to visit data page. Fixed-target results The fixed-target section has four different subsections: Data Summary Expected Runtime Probability Mass Function Cumulative Distribution. The analysis provided in those sections are described as follows: Data Summary provides some statistics on the running time $T(A, f, d, v)$, meaning the function evaluation an algorithm $A$ would require to reach target value $v$ of test function $f$ on dimension $d$. In the following, the indexing parameter in $T$ will be dropped if no ambiguity is created. Assuming a number of independent runs of algorithm $A$ is performed on the tuple of $(f, d, v)$, the set of results from all runs $(t_i)_i$ is considered as a simple random sample of $T$. Here, three tables are provided to summarize the sample $(t_i)_i$ of $T$: Data Overview A screenshot of this table is given as below. As counterintuitive as it may seem, this table contains the overview of the function value observed in a data set. The main reason of showing this table here is to provide the user a quick summary of the range of function value, which is required to play with the following two functionalities. Runtime Statistics at Chosen Target Values A screenshot of this table is given as below. The table is obtained from The user can set the range and the granularity of the results in the box on the left. The table shows fixed-target running times for evenly spaced target values. More precisely, for each tuple of (algorithm $A$, target value $v$, dimension $d$) the table provides 1) successful runs: the number of runs (sample points) of algorithm $A$ in which at least one solution $x$ satisfying $f(x)&gt;v$ has been found 2) sample mean, median, standard deviation 3) sample quantiles: $Q_{2 %}, Q_{5 %}, ldots, Q_{98 %}$ and 4) the expected running time (ERT). Additionally, the user can also download this table in CSV format, or as a LaTeX table. Original Runtime Samples The user interface is similar to the previous except that the sample points $(t_i)_{i}$ of running time $T(A, f, d, v)$ is shown here. Moreover, the user can choose between a long (all sample points are stored in a column) and a wide format (all sample points are stored in a row) for the table. Expected runtime An interactive plot (using shiny package) illustrates the fixed-target running times. An example of this plot is shown as below as below. The interactive plot can be adjusted by a couple of options on the left menu as shown in figure, including showing/hiding mean and/or median values along with standard deviations and scaling axis logarithmically. The user also selects the algorithms to be displayed, the range of target values within which the curves are drawn. The displayed curves can be switched on and off by clicking on the legend on the right of the plot. In addition, the figure can be downloaded in the following format: pdf, eps, svg and png (which also applies to all the plots hereafter). Probability mass function For a selected target value $v$, the histogram of the running time, as displayed below, shows the number of runs $i$ where the running time falls into a given interval $[t,t+1)$, namely $t le t_i(A,f,d,v) &lt; t+1$. The bin size $[t,t+1)$ is automatically determined according to the so-called Freedman-Diaconis rule, which is based on the interquartile range of sample $(t_i)_i$. The user has two options: overlayed display, where all algorithms are displayed in the same plot separated one, where each algorithm is displayed in an individual sub-plot. In addition to the histogram, the empirical probability mass function (see figure below) might be helpful to get a finer look at the shape of the empirical distribution of $T(A,f,d,v)$. The user can opt to show all sample points $(t_i)_{i}$ for each algorithm (which will be plotted as dots), or only the empirical probability mass function itself. It is important to point out that the probability mass function is estimated in a ““continuous’’ manner, where running time samples are considered as $ mathbb{R}$-valued and then a Kernel Density Estimation (KDE) is taken to obtain the curve. Note also that in the figure many sample points seem overlapping, this might be caused by turning (in the data upload part) the efficient mode on, in which the raw data set is trimmed. Cumulative distribution The empirical cumulative distribution function of the running time are computed for target values specified by the user. In addition to showing ECDFs for a single target value, it is recommended to aggregate ECDFs over multiple target, to obtain an overall performance profile for all algorithms. Such a functionality is exemplified below, a set of evenly spaced target value can be generated by specifying the range and step of the target value. In this example, with the following setup, $f_{ min}=0.46$, $f_{ max}=4.91$, and $ Delta f=0.5$, the ECDF curves for target values $0.46,0.96,1.46, ldots, 4.91$ are computed. The aggregation across targets is defined in the following sense: for a set of target values $V$, $r$ number of independent runs on each function, the aggregated ECDF considers running time samples of all target values and runs together. In the upper figure, for algorithm $(30,30)$-vGA (blue curve) this is the case for around $70 %$ of the pairs after $t=2 ,0000$ function evaluations. For algorithm RLS (purple curve) the fraction is $80 %$. Ideally, the best algorithm would sample the maximal function value $f_{ max}$ in the first function evaluation. This algorithm would have a 100% cumulative probability for any running time. In practice, such an algorithm does not exist, but it serves as a theoretical upper bound. Furthermore, this way of performing the aggregation can be leveraged to a set of test functions, namely it is straightforward to define the aggregated profile for a range of functions This functionality is also materizalized in IOHanalyzer. In the example figure, a table of pre-calculated target values are provided for each test function while all 23 test functions are considered here by default. This table could also be customized by 1) downloading the current table (in a CSV-like format) 2) modifying it according to user’s preference 3) uploading the modified table again. The plot on the right will be re-computed upon uploading a new table of targets. Note that, please keep the format of the example table while editing it.",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/GraphicUserInterface/",
    "relUrl": "/IOHanalyzer(Post-Processing)/GraphicUserInterface/"
  },
  "6": {
    "id": "6",
    "title": "IOHanalyzer",
    "content": "IOHanalyzer In this page, an example on IOHanalyzer is given based on the benchmark data generated in the previous section. In case it occurs to be time-consuming for the reader to execute the benchmarking example, the exactly same data set can also be downloaded from the site. We provide and maintain two versions of the IOHanalyzer package: a CRAN (Comprehensive R Archive Network) version R package a latest stable version that is hosted on Github page. The CRAN version can be obtained by: &gt;install.packages(&#39;IOHanalyzer&#39;) To install the latest stable version, pkg{devtools} is required again: &gt;devtools::install_github(&#39;IOHprofiler/IOHanalyzer&#39;) And please import the package after installation: &gt;library(&#39;IOHanalyzer&#39;) Note that those two versions only differ in some aesthetic aspects of the plotting method, which undergos a continuous and constant improvement. The example here is mainly two-fold: the usage of the programming interface and the Graphical User Interface.",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/",
    "relUrl": "/IOHanalyzer(Post-Processing)/"
  },
  "7": {
    "id": "7",
    "title": "IOHexperimenter",
    "content": "IOHexperimenter The benchmarking platform for Iterative Optimization Heuristics (IOHs). Documentation: https://arxiv.org/abs/1810.05281 Wiki page: https://iohprofiler.github.io/IOHanalyzer General Contact: iohprofiler@liacs.leidenuniv.nl Mailing List: https://lists.leidenuniv.nl/mailman/listinfo/iohprofiler IOHexperimenter provides: A framework for straightforward benchmarking of any iterative optimization heuristic A suite consisting of 23 pre-made Pseudo-Boolean benchmarking function, with easily accessible methods for adding custom functions and suites Logging methods to effortlesly store benchmarking data in a format compatible with IOHanalyzer, with future support for additional data logging options (Soon to come:) A framework which significantly simplifies algorithm design IOHexperimenter is built on: C++ (tested on gcc 5.4.0) boost.filesystem library for logging files. IOHexperimenter is available for: C++ R package https://github.com/IOHprofiler/IOHexperimenter/tree/R Python interface (soon to come) Java interface (soon to come) Using IOHexperimenter Running Experiments The IOHexperimenter has been built on C++ and tested on complier gcc 5.4.0. To use the logging of csv output files, boost.filesystem library is required (To install boost library, please visit https://www.boost.org). Using by C++ If you are using the tool for the first time, please download or clone this branch and run make at the root directory of the project. After running make to compile, object files will be generated in build/c/obj three exectuable files will be generated in build/c/bin Afterwards, you can use the folder build/c and use the Makefile therein for your experiments. For more details of how to use the C++ version, please visit this page. Using by R For the use of R, please visit R branch. Creating test problems Benchmarking problems in IOHexperimenter are easy to create yourself. We provide support for any input type and any number of real-valued objectives. For a more detailed guidline of how to define a benchmarking problem within IOHexperimenter, please visit this page. Configuring test suites Suites are collections of benchmarking problems. By including problems into a suite, it is easier for users to maintain their experiments. If you create a set of similar problems, it is recommended to create a suite to collect them together, which can be done effortlesly within the IOHexperimenter. For detailed steps of creating and using suites, please visit this page.",
    "url": "http://localhost:4000/IOHexperimenter/",
    "relUrl": "/IOHexperimenter/"
  },
  "8": {
    "id": "8",
    "title": "Loggers",
    "content": "Loggers IOHexperimenter allows multiple ways to track optimization process, IOHprofiler_observer defines the triggers to track, and class of loggers implements details of tracking process. For now, we supply IOHprofiler_csv_logger to store function evaluations in csv files.",
    "url": "http://localhost:4000/IOHexperimenter/Loggers/",
    "relUrl": "/IOHexperimenter/Loggers/"
  },
  "9": {
    "id": "9",
    "title": "Observer",
    "content": "IOHprofiler_observer IOHprofiler_observer defines triggers of recording evaluations, and logger classes inherit it to set up the time of recording. Four strategies of recording evaluations are available, complete tracking, provides the highest granularity, by storing information for each function evaluation. Use set_complete_flag(true) to enable this strategy, interval tracking, stores information for each $ tau$-th function evaluation. Use set_interval() to set $ tau &gt; 0$ to enable this stragety. target-based tracking, stores information for each iteration in which the best-so-far fitness improved. Use set_update_flag(true) to enbale this stragety. time-based tracking, stores information when the user-specified running time budgets are reached. These budget are evenly spaced in log-10 scale, taking the form $v10^i, i=0,1,2,…$ or $10^{i/t}, i = 0,1,2,…$ . You can use set_time_points(v, t) to set $v$ and $t$.",
    "url": "http://localhost:4000/IOHexperimenter/Loggers/Observer/",
    "relUrl": "/IOHexperimenter/Loggers/Observer/"
  },
  "10": {
    "id": "10",
    "title": "Problems",
    "content": "Problems Definition F1: OneMax (Hamming Distance） It asks to optimize $OM:{0,1} rightarrow [0..n], x mapsto sum_{i=1}^n{x_i}$. The problem has a very smooth and non-deceptive fitness landscape. Due to the well-known coupon collector effect, it is relatively easy to make progress when the function values are small, and the probability to obtain an improving move decreases considerably with increasing function value. F2: LeadingOnes The problem asks to maximize the function $LO:{0,1}^n to [0..n], x mapsto max {i in [0..n] mid forall {j} le {i}: x_j=1} = sum_{i=1}^n{ prod_{j=1}^i{x_i}}$, which counts the number of initial ones. F3: A Linear Function with Harmonic Weights The problem is a linear function $f:{0,1}^n to mathbb{R}, x mapsto sum_{i} i x_i$ with harmonic weights. F4-F17: The W-model The W-model comprises 4 basic transformations, each coming with different instances. We use $W( cdot, cdot, cdot, cdot)$ to denote the configuration chosen in our benchmark set. Reduction of dummy variables $W(k, ast, ast, ast)$: a reduction mapping each string $(x_1, ldots, x_n)$ to a substring $(x_{i_1}, ldots, x_{i_k})$ for randomly chosen, pairwise different $i_1, ldots, i_k in [n]$. Neutrality $W( ast, mu, ast, ast)$: The bit string $(x_1, ldots,x_n)$ is reduced to a string$(y_1, ldots,y_m)$ with $m:=n/ mu$, where $ mu$ is a parameter of the transformation. For each $i in [m]$ the value of $y_i$ is the majority of the bit values in a size-$ mu$ substring of $x$. More precisely, $y_i=1$ if and only if there are at least $ mu/2$ ones in the substring $(x_{(i-1) mu+1},x_{(i-1) mu+2}, ldots,x_{i mu})$. When ${n/ mu} notin { mathbb{N}}$, the last bits of $x$ are simply copied to$y$. In our assessment, we regard only the case $ mu=3$. Epistasis $W( ast, ast, nu, ast)$ The idea of epistasis is to introduce local perturbations to the bit strings. To this end, a string $x=(x_1, ldots,x_n)$ is divided into subsequent blocks of size $ nu$. Using a permutation $e_{ nu}:{0,1}^{ nu} to {0,1}^{ nu}$, each substring $(x_{(i-1) nu+1}, ldots,x_{i nu})$ is mapped to another string $(y_{(i-1) nu+1}, ldots,y_{i nu})=e_{ nu}((x_{(i-1) nu+1}, ldots,x_{i nu}))$. The permutation $e_{ nu}$ is chosen in a way that Hamming-1 neighbors $u,v in {0,1}^{ nu}$ are mapped to strings of Hamming distance at least $ nu-1$. In our evaluation, we use $ nu=4$ only. Fitness perturbation $W( ast, ast, ast,r)$. With this transformation we can determine the ruggedness and deceptiveness of a function. Unlike the previous transformations, this perturbation operates on the function values, not on the bit strings. To this end, a emph{ruggedness} function $r:{f(x) mid {x} in {0,1}^n }=:V to {V}$ is chosen. The new function value of a string $x$ is then set to $r(f(x))$ , so that effectively the problem to be solved by the algorithm becomes ${r} circ {f}$. We use the following three ruggedness functions. $r_1:[0..s] to [0.. lceil{s/2} rceil+1$ with $r_1(s)= lceil {s/2} rceil +1$ and $r_1(i)= lfloor {i/2} rfloor+1$ for $i&lt;s$ and even s, and $r_1(i)= lceil {i/2} rceil+1$ for $i&lt;s$ and odd $s$. This function maintains the order of the search points $r_2:[0..s] to [0..s]$ with $r_2(s)=s$, $r_2(i)=i+1$ for $i equiv {s {mod} 2}$ and $i&lt;s$, and $r_2(i)= max{ {i-1,0}}$ otherwise. This function introduces moderate ruggedness at each fitness level. $r_3:[0..s] to [0..s]$ with $r_3(s)=s$ and $r_3(s-5j+k)=s-5j+(4-k)$ for all $j in {[s/5]}$ and $k { in} [0..4]$ and $r_3(k)=s - (5 lfloor {s/5} rfloor - 1 )- k$ for $k in [0..s - 5 lfloor {s/5} rfloor -1]$. With this function the problems become quite deceptive, since the distance between two local optima implies a difference of $5$ in the function values. F4-F17 present superpositions of individual W-model transformations to the OneMax (F1) and the LeadingOnes (F2) problem. Precisely, F4-F17 are F4: $ text{OneMax} + W([n/2],1,1,id)$ F5: $ text{OneMax} + W([0.9n],1,1,id)$ F6: $ text{OneMax} + W([n], mu=3,1,id)$ F7: $ text{OneMax} + W([n],1, nu=4,id)$ F8: $ text{OneMax} + W([n],1,1,r_1)$ F9: $ text{OneMax} + W([n],1,1,r_2)$ F10: $ text{OneMax} + W([n],1,1,r_3)$ F11: $ text{LeadingOnes} + W([n/2],1,1,id)$ F12: $ text{LeadingOnes} + W(0.9n,1,1,id)$ F13: $ text{LeadingOnes} + W([n], mu=3,1,id)$ F14: $ text{LeadingOnes} + W([n],1, nu=4,id)$ F15: $ text{LeadingOnes} + W([n],1,1,r_1)$ F16: $ text{LeadingOnes} + W([n],1,1,r_2)$ F17: $ text{LeadingOnes} + W([n],1,1,r_3)$ F18: Low Autocorrelation Binary Sequences (LABS) The Low Autocorrelation Binary Sequences (LABS) problem poses a non-linear objective function over a binary sequence space, with the goal to maximize the reciprocal of the sequence’s autocorrelation: $ frac{n^2}{2E(S)} text{ with } E(S) = sum_{k=1}^{n-1} left( sum_{i=1}^{n-k}s_i cdot s_{i+k} right)^2$ where the sequence is of length n $S:= left(s_1, ldots,s_n right)$ with $s_i= pm 1$ . To obtain a pseudo-Boolean problem, we use the straightforward interpretation $s_i=2x_i-1$ for all $i in [n]$. F19-F21: The Ising Model The classical Ising model cite{Ising_Barahona1982} considers a set of spins placed on a regular lattice $G=([n],E)$, where each edge $(i,j) in {E}$ is associated with an interaction strength $J_{ij}$. Given a configuration of $n$ spins, $S:= left(s_1, ldots,s_n right)$, this problem poses a quadratic function, representing the system’s energy and depending on its structure $J_{ij}$. Assuming zero external magnetic fields and using $s_i=2x_i-1$ we obtain the following pseudo-Boolean maximization problem: $[ISING:] sum limits_{ {i,j} in {E}} left[x_{i}x_{j} - left(1-x_{i} right) left(1-x_{j} right) right] $ F19 considers the one-dimensional ring, F20 considers the two-dimensional torus, and F21 considers the two-dimensional triangular. F22: Maximum Independent Vertex Set Given a graph $G=([n],E)$ , an independent vertex set is a subset of vertices where no two vertices are linked by an edge. A maximum independent vertex set (MIVS) is defined as an independent subset Given a graph $V^{ prime} subset [n]$ having largest possible size. $[MIVS:] sum_i x_i ; textrm{s.t.} ; sum_{i &lt; j} x_i x_j e_{ij} = 0~$. F23: N-Queens Problem The N-queens problem (NQP) is defined as the task to place N queens on an ${N} times{N}$ chessboard in such a way that they cannot attack each other. Selection of upper problems is suggested in the paper https://doi.org/10.1145/3319619.3326810 .",
    "url": "http://localhost:4000/Benchmark/Problems/",
    "relUrl": "/Benchmark/Problems/"
  },
  "11": {
    "id": "11",
    "title": "Programming Interface",
    "content": "Programming Interface Data structure and manipulation Retrieving performance data Plotting Data structure and manipulation Here, it is assumed that the data to be loaded follow exactly the aforementioned format regulation. A method DataSetList is provided to load the data: &gt; dsList &lt;- DataSetList(&#39;./data/RLS&#39;) Processing ./data/RLS/IOHprofiler_f1_i1.info ... algorithm RLS... 25 instances on f1 16D... 25 instances on f1 100D... ... The return value of method DataSetList is a S3 object, that is inherited from the list class. Consequently, list object can be sliced, indexed and printed as with lists: &gt; dsList DataSetList: 1: DataSet(RLS on f1 16D) 2: DataSet(RLS on f1 100D) 7: DataSet(RLS on f23 16D) 8: DataSet(RLS on f23 100D) &gt; dsList[1:3] DataSetList: 1: DataSet(RLS on f1 16D) 2: DataSet(RLS on f1 100D) 3: DataSet(RLS on f19 16D) &gt; dsList[[1]] DataSet(RLS on f1 16D) In addition, the summary method is implemented to show some basic information: &gt; summary(dsList) funcId DIM algId datafile comment 1 1 16 RLS ./data/RLS/data_f1/IOHprofiler_f1_DIM16_i1.dat % 2 1 100 RLS ./data/RLS/data_f1/IOHprofiler_f1_DIM100_i1.dat % 3 19 16 RLS ./data/RLS/data_f19/IOHprofiler_f19_DIM16_i1.dat % 4 19 100 RLS ./data/RLS/data_f19/IOHprofiler_f19_DIM100_i1.dat % 5 2 16 RLS ./data/RLS/data_f2/IOHprofiler_f2_DIM16_i1.dat % 6 2 100 RLS ./data/RLS/data_f2/IOHprofiler_f2_DIM100_i1.dat % 7 23 16 RLS ./data/RLS/data_f23/IOHprofiler_f23_DIM16_i1.dat % 8 23 100 RLS ./data/RLS/data_f23/IOHprofiler_f23_DIM100_i1.dat % Note that, column funcId stands for the numbering (ID) of test functions and algId is the identifier of the algorithm that is tested. Those columns (also DIM) are the attribute of list object, which are directly retrieved from the meta data (*.info files). Therefore, it is important to keep the meta data correct if it were prepared by the user manually. All the attributes DataSetList object are listed as follows: &gt; attributes(dsList) $class [1] &quot;list&quot; &quot;DataSetList&quot; $DIM [1] 16 100 16 100 16 100 16 100 $funcId [1] 1 1 19 19 2 2 23 23 $algId [1] &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; When subsetting (filtering) is needed for list, attributes DIM, funcId and algId can be used as follows: &gt; subset(dsList, DIM == 16, funcId == 1) DataSetList: 1: DataSet(RLS on f1 16D) &gt; subset(dsList, DIM == 16, algId != &#39;RLS&#39;) DataSetList: Now we could load the data files of the $(1, lambda)$-GA algorithm in the same way: &gt; dsList_ga &lt;- DataSetList(&#39;./data/self_GA&#39;, verbose = FALSE) Here, the argument verbose is set to FALSE to hide the prompting message. As with the R list, DataSetList objects can be combined together: &gt; dsList &lt;- c(dsList, dsList_ga) Each element of list is a S3 object of type DataSet, which is again inherited from the list class. &gt; ds &lt;- dsList[[1]] &gt; ds DataSet(RLS on f1 16D) &gt; summary(ds) DataSet Object: Algorithm: RLS Function ID: 1 Dimension: 16D 25 instance found: 1,1,1,1,1,2,2,...,4,4,4,5,5,5,5,5 runtime summary: algId target mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% ERT runs ps 1: RLS 4 1.00 1 0.000000 1 1 1 1 1 1 1 1 1 1.00 25 1 2: RLS 5 1.04 1 0.200000 1 1 1 1 1 1 1 1 1 1.04 25 1 3: RLS 6 1.28 1 1.208305 1 1 1 1 1 1 1 2 2 1.28 25 1 11: RLS 14 21.00 21 9.165151 5 5 7 14 18 26 34 37 37 21.00 25 1 12: RLS 15 29.16 29 10.466932 12 12 15 18 26 36 40 48 48 29.16 25 1 13: RLS 16 46.48 48 21.652790 13 13 19 26 42 58 71 83 83 46.48 25 1 function value summary: algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: RLS 1 25 7.68 7 1.7729448 4.48 5.2 6.0 7 7 9 10.0 10.8 11.00 2: RLS 2 25 8.28 8 1.6206994 5.48 6.0 6.4 7 8 9 10.0 10.8 11.52 3: RLS 3 25 8.76 9 1.5885003 5.48 6.2 7.0 8 9 10 10.6 11.0 11.52 64: RLS 2511 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 65: RLS 2818 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 66: RLS 3162 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 Attributes: names, class, suite, funcId, DIM, algId, algInfo, comment, datafile, instance, maxRT, finalFV, format, maximization In the summary method, the data set is summarized in two perspectives: Fixed-target perspective: the method looks for the first hitting time, that is the number of function evaluations an algorithm takes to reach a target function value (target above) for the first time. The target values are automatically determined and evenly spaced in the observed range. Some basic statistics on the running time sample are calculated for each target value: mean, median, standard deviation (sd), percentiles (2% 5% 10% …), the expected running time (ERT) and success rate (ps, the ratio of successful runs our of all the independent runs). Fixed-budget perspective: the method looks for the best function value reached by the algorithm, when a specific number of function evaluations (budget) are taken (runtime above). The budget values are automatically determined and evenly spaced in the observed range. Roughly the same set of statistics are provided as the fixed-target perspective. In the ds object, two matrices are always stored for those two perspectives explained above: ds$RT: running time samples in the fixed-target perspective and ds$FV: function value samples in the fixed-budget perspective. Note that, when the parameter tracking is enabled, the parameter of interest is also arranged in the fixed-target perspective and is appended to ds object. For instance, if mutation_rate is the parameter name given to the benchmark, the parameter can be obtained by ds$mutation_rate. We could take a glimpse at those two matrices: &gt; head(ds$RT) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 2 1 1 6 1 1 1 1 1 1 1 1 2 1 1 7 1 1 7 1 1 1 1 1 3 1 1 5 1 1 8 1 1 8 1 3 2 2 1 4 1 1 7 2 1 10 2 1 9 1 4 3 3 4 5 1 1 8 4 2 11 3 1 Here, the column names are the target values and in the example below, those are the budget values: &gt; head(ds$FV) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] 1 11 7 7 7 8 6 9 9 5 7 8 4 7 11 2 11 7 8 8 8 6 9 10 6 8 9 5 8 12 3 11 8 9 9 8 7 10 10 6 8 9 5 9 12 4 12 9 10 9 9 8 10 11 6 9 10 5 9 13 5 12 9 10 9 10 9 10 12 7 10 10 5 10 14 6 13 9 11 10 10 10 10 13 7 11 11 5 10 14 Retrieving performance data For both DataSet and DataSetList objects, the overview of the observed running time/function value can obtained using: &gt; get_RT_overview(subset(dsList, algId == &#39;RLS&#39;)) Algorithm DIM fID miminal runtime maximal runtime runs Budget 1: RLS 16 1 1 96 25 16000 2: RLS 100 1 1 1128 25 100000 3: RLS 16 19 1 52 25 16000 4: RLS 100 19 1 570 25 100000 5: RLS 16 2 1 255 25 16000 6: RLS 100 2 1 6546 25 100000 7: RLS 16 23 1 67 25 16000 8: RLS 100 23 1 641 25 100000 &gt; get_RT_overview(ds) algId DIM funcId miminal runtime maximal runtime runs Budget 1: RLS 16 1 1 96 25 16000 Here, Budget indicates the maximal allowable budget that is given when running the experiment while maximal runtime is the maximal observed running time in each triplet of (algId, DIM, funcId). For the function values, the similar methods are also implemented: &gt; get_FV_overview(subset(dsList, algId == &#39;RLS&#39;)) algId DIM funcId worst recorded worst reached best reached mean reached median reached runs succ budget 1: RLS 16 1 4 16 16 16.00 16 25 25 16000 2: RLS 100 1 44 100 100 100.00 100 25 25 100000 3: RLS 16 19 8 20 32 25.60 24 25 1 16000 4: RLS 100 19 92 152 172 162.56 164 25 3 100000 5: RLS 16 2 0 16 16 16.00 16 25 25 16000 6: RLS 100 2 0 100 100 100.00 100 25 25 100000 7: RLS 16 23 -100 3 4 3.24 3 25 6 16000 8: RLS 100 23 -1868 7 9 8.20 8 25 9 100000 It is important to distinguish some columns in the example here: worst recorded stands for the worst (smallest) function value observed in all independent runs for each case of (algId, DIM, funcId). In contrast, worst reached means the smallest value reached in the last iteration (across independent runs) of the algorithm while best reached the largest value. runs gives the total number of independent runs in each case while succ is the number of runs where the corresponding best reached is hit. Note that, in our naming convention of methods, RT is always the abbreviation of running time and FV is for function value (the same below). To get a data summary at arbitrary running time/function value, two methods, get_RT_summary and get_FV_summary are implemented. Let use the object ds (defined before) to illustrate the usage: &gt; ds DataSet(RLS on f1 16D) &gt; get_RT_summary(ds, ftarget = c(5, 10, 16)) algId target mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% ERT runs ps 1: RLS 5 1.04 1 0.200000 1 1 1 1 1 1 1 1 1 1.04 25 1 2: RLS 10 5.24 5 3.455431 1 1 1 2 5 6 10 12 12 5.24 25 1 3: RLS 16 46.48 48 21.652790 13 13 19 26 42 58 71 83 83 46.48 25 1 &gt; get_FV_summary(ds, runtime = c(10, 50, 100)) algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: RLS 10 25 11.52 12 1.5307950 8.48 9 9.4 11 12 13 13 13.8 14 2: RLS 50 25 15.64 16 0.4898979 15.00 15 15.0 15 16 16 16 16.0 16 3: RLS 100 25 16.00 16 0.0000000 16.00 16 16.0 16 16 16 16 16.0 16 The input for the argument ftarget and runtime should be provided by the user. In this example, three values are chosen arbitrarily in the corresponding range of running time/function value (cf. the first lines of get_RT_overview and get_FV_overview above). Furthermore, using the magrittr package (please install it if necessary), it is possible to chain all the methods introduced so far, making the code snippet more readable: &gt; library(magrittr) &gt; dsList %&gt;% + subset(DIM == 100, algId == &#39;RLS&#39;, funcId == 19) %&gt;% + get_FV_summary(runtime = seq(1, 5000, length.out = 5)) DIM funcId algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: 100 19 RLS 1.00 25 104.16 104 8.284926 92 92.0 93.6 100 104 108 116.8 120 120 2: 100 19 RLS 1250.75 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 3: 100 19 RLS 2500.50 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 4: 100 19 RLS 3750.25 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 5: 100 19 RLS 5000.00 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 In addition, it is also straightforward to retrieve the raw sample points of the running time/function value, using the following methods, get_RT_sample and get_FV_sample: &gt; get_RT_sample(ds, ftarget = 10, output = &#39;long&#39;) algId target run RT 1: RLS 10 1 1 2: RLS 10 2 8 3: RLS 10 3 4 23: RLS 10 23 12 24: RLS 10 24 1 25: RLS 10 25 8 &gt; get_FV_sample(ds, runtime = c(5, 20), output = &#39;wide&#39;) algId runtime run.1 run.2 run.3 run.4 run.5 run.6 run.7 run.8 run.9 run.10 run.11 run.12 1: RLS 5 12 9 10 9 10 9 10 12 7 10 10 5 2: RLS 20 16 12 15 14 14 13 14 15 14 12 14 13 Note that, all methods in this sub-section return a data.table object from the data.table package. Plotting To visualize the benchmark data, a collection of plotting methods are implemented in iohana. In this section, we shall provide examples on some important plots using the data set in the last section. Here, only the plotting method for the fixed-target perspective is shown because the same set of methods are implemented for the fixed-budget perspective. Firstly, the progression of the function value is plotted against the running time: &gt; ds_plot &lt;- subset(dsList, DIM == 16, funcId == 1) &gt; Plot.RT.Single_Func(ds_plot) The data sets on 16D, function F1 are plotted here, which is shown in the following figure. Note that, a interactive plot is created as the plotly library is used here by default. The static plotting library ggplot2 can also be selected by setting argument backend = &#39;ggplot2&#39; (this is only a difference in the plotting backend and thus it will not be demonstrated here). In the figure, the standard deviation of the running time is also drawn. &gt; ?Plot.RT.Single_Func In addition, the previous plot can be grouped by functions, using Plot.RT.Multi_Func methods. The example is shown below. &gt; Plot.RT.Multi_Func(ds_plot, scale.ylog = T) Given a target value, Plot.RT.Histogram methods renders the histogram of the running time required to reach this target value. Taking the data set on F23 and 16D as an example, it is important to view the range of function values through method get_FV_overview as shown in the previous section: &gt; ds &lt;- subset(dsList, DIM == 16, funcId == 23) &gt; get_FV_overview(ds) algId DIM funcId worst recorded worst reached best reached ... Budget 1: RLS 16 23 -100 3 4 ... 16000 2: self_GA 16 23 -96 4 4 ... 16001 Then, we choose a target value -3 that is close to the best reached value and plot the histogram. &gt; Plot.RT.Histogram(ds, ftarget = -3, plot_mode = &#39;subplot&#39;) The argument plot_mode = &#39;subplot&#39; will create a separate sub-plot for each algorithm in the data set. In addition, the empirical density function of the running time, that is estimated by the Kernel Density Estimation (KDE) method, can be generated by method Plot.RT.PMF. &gt; Plot.RT.PMF(ds, ftarget = -3, show.sample = TRUE) Finally, it is also crucial to look at the Empirical Cumulative Distribution function (ECDF) of the running time. For this purpose, three methods are implemented for different levels of data aggregation: Plot.RT.ECDF_Per_Target: it only compares the ECDF of algorithms on a single target value, e.g., &gt; Plot.RT.ECDF_Per_Target(ds, ftarget = -1) Plot.RT.ECDF_Single_Func: it takes as input an array of target values (controlled by arguments fstart, fstop, fstep) and aggregates the ECDF over those targets, e.g., &gt; Plot.RT.ECDF_Single_Func(ds, fstart = -92, fstop = 4, fstep = 10) Plot.RT.ECDF_Multi_Func: it, in addition, aggregates different target values over all test function in a data set. To demonstrate its usage, let’s take the data set on 100D and check the overview of the function values. Then three target values are chosen manually for each function, which are collected in a list object. The resulting plot is shown i. &gt; ds &lt;- subset(dsList, DIM == 100) &gt; get_FV_overview(ds) algId DIM funcId worst recorded worst reached best reached ... budget 1: RLS 100 1 44 100 100 ... 100000 2: RLS 100 19 92 152 172 ... 100000 3: RLS 100 2 0 100 100 ... 100000 4: RLS 100 23 -1868 7 9 ... 100000 5: self_GA 100 1 38 98 100 ... 100001 6: self_GA 100 19 72 164 192 ... 100001 7: self_GA 100 2 0 39 100 ... 100001 8: self_GA 100 23 -1761 7 10 ... 100001 &gt; ftarget &lt;- list(`1` = c(80, 90, 100), + `2` = c(80, 90, 100), + `19` = c(180, 190, 200), + `23` = c(0, 5, 10)) &gt; Plot.RT.ECDF_Multi_Func(ds, ftarget)",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/ProgrammingInterface/",
    "relUrl": "/IOHanalyzer(Post-Processing)/ProgrammingInterface/"
  },
  "12": {
    "id": "12",
    "title": "Transformation",
    "content": "Transformation Instead of testing one particular problem $f$ only, the user can choose to run experiments on several problem instances that are obtained from $f$ through a set of transformations. In its most general form,IOHprofiler currently offers to return to the algorithm the values as $af( sigma (x { oplus} z)) + b$, where $a$ is a multiplicative shift of the function value, $b$ is an additive shift of the function value, ${ oplus}z: {0,1}^n to {0,1}^n, (x_1, … ,x_n) mapsto ((x_1 + z_1) {mod} 2, … ,(x_n + z_n) {mod} 2)$ is an XOR-shift of the search point, $ mathcal{S}^n to mathcal{S}^n, (x_1, … ,x_n) mapsto (x_{ sigma(1)}, … ,x_{ sigma(n)})$ is a permutation of the search point. Note here that, in abuse of notation, we identify the permutation ${ sigma}: [1..n] to [1..n]$ with the here-defined re-ordering of the bit string. In practical, we applied $af(x { oplus} z) + b$ to instance 2-50, and $af( sigma (x)) + b$ to instance 51-100. $a in [ frac{1}{5},5]$ and $b in [-1000,1000]$ are randomly generated.",
    "url": "http://localhost:4000/Benchmark/Transformation/",
    "relUrl": "/Benchmark/Transformation/"
  },
  "13": {
    "id": "13",
    "title": "About",
    "content": "IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics IOHprofiler is a new tool for analyzing and comparing iterative optimization heuristics. It consists of two part: IOHexperimenter and IOHanalyzer. IOHexperimenter provides a tool for: A framework for straightforward benchmarking of any iterative optimization heuristic A suite consisting of 23 pre-made Pseudo-Boolean benchmarking function, with easily accessible methods for adding custom functions and suites Logging methods to effortlesly store benchmarking data in a format compatible with IOHanalyzer, with future support for additional data logging options (Soon to come:) A framework which significantly simplifies algorithm design IOHanalyzer provides: a web-based interface to analyze and visualize the empirical performance of IOHs interactive plot statistical evaluation report generation R programming interfaces in the backend Source source code : https://github.com/IOHprofiler Documentation: https://arxiv.org/abs/1810.05281 Wiki page: https://iohprofiler.github.io/IOHanalyzer Bug reports (IOHanalyzer): https://github.com/IOHprofiler/IOHAnalyzer/issues Online service (IOHanalyzer): http://iohprofiler.liacs.nl General Contact: iohprofiler@liacs.leidenuniv.nl Mailing List: https://lists.leidenuniv.nl/mailman/listinfo/iohprofiler Contact If you have any questions, comments, suggestions or pull requests, please don’t hesitate contacting us IOHprofiler@liacs.leidenuniv.nl! When using IOHprofiler and parts thereof, please kindly cite this work as Carola Doerr, Hao Wang, Furong Ye, Sander van Rijn, Thomas Bäck: IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics, arXiv e-prints:1810.05281, 2018. @ARTICLE{IOHprofiler, author = {Carola Doerr and Hao Wang and Furong Ye and Sander van Rijn and Thomas B{ &quot;a}ck}, title = {IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics}, journal = {arXiv e-prints:1810.05281}, archivePrefix = &quot;arXiv&quot;, eprint = {1810.05281}, year = 2018, month = oct, keywords = {Computer Science - Neural and Evolutionary Computing}, url = {https://arxiv.org/abs/1810.05281} }",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "14": {
    "id": "14",
    "title": "News",
    "content": "News Welcome to IOHprofiler",
    "url": "http://localhost:4000/News/",
    "relUrl": "/News/"
  }
  
}
