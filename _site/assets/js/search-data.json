{
  "0": {
    "id": "0",
    "title": "",
    "content": "404 Page not found :( The requested page could not be found.",
    "url": "http://localhost:4000/404/",
    "relUrl": "/404/"
  },
  "1": {
    "id": "1",
    "title": "AddingProblems",
    "content": "Adding Problems IOHprofiler_problem class (src) presents a common template class for problems using in IOHexperimenter, users can add new problems by creating new class inheriting IOHprofiler_problem. IOHprofiler_problem supports problems with variables being arbitrary but unique type and objectives as real numbers. Taking OneMax as an example, InputType, which is the type of variables, set as int. With must header file, problem_id, instance_id, problem_name, problem_type and number_of_objectives must be set by the constructor function. But to specify the problem to be tested, interfaces for setting number_of_variables, lowerbound and upperbound of variables, best_variables. optimal is an optional setting, since for some problem optimal is unknown. Definition of the problem is implemented in the function internal_evaluate, which will be used in the evaluate function of IOHprofiler_problem. The argument of the function is input variables, and the function returns a vector of objectives. #include &quot;../IOHprofiler_problem.hpp&quot; class OneMax : public IOHprofiler_problem&lt;int&gt; { public: OneMax() { IOHprofiler_set_problem_id(1); IOHprofiler_set_instance_id(1); IOHprofiler_set_problem_name(&quot;OneMax&quot;); IOHprofiler_set_problem_type(&quot;pseudo_Boolean_problem&quot;); IOHprofiler_set_number_of_objectives(1); } OneMax(int instance_id, int dimension) { IOHprofiler_set_problem_id(1); IOHprofiler_set_instance_id(instance_id); IOHprofiler_set_problem_name(&quot;OneMax&quot;); IOHprofiler_set_problem_type(&quot;pseudo_Boolean_problem&quot;); IOHprofiler_set_number_of_objectives(1); Initilize_problem(dimension); } void Initilize_problem(int dimension) { IOHprofiler_set_number_of_variables(dimension); IOHprofiler_set_lowerbound(0); IOHprofiler_set_upperbound(1); IOHprofiler_set_best_variables(1); IOHprofiler_set_optimal((double)dimension); }; std::vector&lt;double&gt; internal_evaluate(std::vector&lt;int&gt; x) { std::vector&lt;double&gt; y; int n = x.size(); int result = 0; for(int i = 0; i != n; ++i) { result += x[i]; } y.push_back((double)result); return y; }; }",
    "url": "http://localhost:4000/IOHexperimenter/AddingProblems/",
    "relUrl": "/IOHexperimenter/AddingProblems/"
  },
  "2": {
    "id": "2",
    "title": "Benchmark",
    "content": "Benchmark PBO_suite : 23 pseudo-Boolean Problems.",
    "url": "http://localhost:4000/Benchmark/",
    "relUrl": "/Benchmark/"
  },
  "3": {
    "id": "3",
    "title": "Getting Started",
    "content": "Getting Started Getting started by C Getting started by Python Getting started by R Getting started by C To use all source files of IOHexperimenter directly by C++, an example file is supplied. Experiments on specific problem. A number of problems have been provided by IOHprofiler, and each of them has been implemented as seperate class (visit here). Users can operate on specific problem by declaring the corresponding class. For instance (_run_problem funtion in the file, we are working on OneMax, a OneMax class variable ‘om’ is declared and the dimension set as 1000. To record the optimization evaluations of the algorithm, a logger is added to output csv files logging evaluation information during optimization process. The logger is an optional setting for users, users can handle evaluations by themselves and omit logger settings. Following codes present an evolutionary algorithm with static mutation rate. With the om variable, om.evaluate(x) returns fitness of the solution x, which also executes the logging operation if logger is added. In addtion, a member function IOHprofiler_hit_optimal() is used to detect if the optimal of om is found. void _run_problem() { OneMax om; int dimension = 1000; om.Initilize_problem(dimension); std::vector&lt;int&gt; time_points{1,2,5}; IOHprofiler_csv_logger logger(&quot;./&quot;,&quot;run_problem&quot;,&quot;EA&quot;,&quot;EA&quot;,true,true,2,time_points,3); om.addCSVLogger(logger); std::vector&lt;int&gt; x; std::vector&lt;int&gt; x_star; std::vector&lt;double&gt; y; double best_value; double mutation_rate = 1.0/dimension; x = Initialization(dimension); copyVector(x,x_star); y = om.evaluate(x); best_value = y[0]; while(!om.IOHprofiler_hit_optimal()) { copyVector(x_star,x); if(mutation(x,mutation_rate)) { y = om.evaluate(x); } if(y[0] &gt; best_value) { best_value = y[0]; copyVector(x,x_star); } } } Experiments on suites (collections of problems). If users are planning to test an algorithm on several problems, suite implements an interface to multiple problems. In addition, with a suite and logger, users can handle the output of experiment on different problems with the same configuration. For instance (_run_suite funtion in the [file]https://github.com/IOHprofiler/IOHexperimenter/blob/NewStructure/src/build/C/IOHprofiler_experiment.cpp), We plan to test the first two problems in PBO_suite in dimension 100,200,300 without any transformation, a suite pbo is declared and assigned. To record the optimization evaluations of the algorithm, a logger is added to output csv files logging evaluation information during optimization process. The logger is an optional setting for users, users can handle evaluations by themselves and omit logger settings. After wards, we can get and test problem from the suite, evaluation_algorithm is a function used to solve problems, you can find it here. void _run_suite() { std::vector&lt;int&gt; problem_id = {1,2}; std::vector&lt;int&gt; instance_id ={1}; std::vector&lt;int&gt; dimension = {100,200,300}; PBO_suite pbo(problem_id,instance_id,dimension); std::vector&lt;int&gt; time_points{1,2,5}; IOHprofiler_csv_logger logger1(&quot;./&quot;,&quot;run_suite&quot;,&quot;EA&quot;,&quot;EA&quot;,true,true,2,time_points,3); pbo.addCSVLogger(logger1); std::shared_ptr&lt;IOHprofiler_problem&lt;int&gt;&gt; problem; int index = 0; while(index &lt; pbo.get_size()){ problem = pbo.get_next_problem(index); evolutionary_algorithm(problem); index++; } } Experiments with configuration IOHprofiler_experimenter provides a pre-defined structure for using IOHexperimenter. By using the class, users can concern on algorithm and maintain settings of experiments with configuration files. For example, an evolutionary algorithm has been implemented as below (argument of the function must follow the same). A IOHprofiler_problem is the argument of the function, and the algorithm achieves fitness by the statement y = problem-&gt;evaluate(x);. std::vector&lt;int&gt; Initialization(int dimension) { std::vector&lt;int&gt; x; x.reserve(dimension); for(int i = 0; i != dimension; ++i){ x.push_back(rand()% 2); } return x; }; int mutation(std::vector&lt;int&gt; &amp;x, double mutation_rate) { int result = 0; int n = x.size(); for(int i = 0; i != n; ++i) { if(rand() / double(RAND_MAX) &lt; mutation_rate) { x[i] = (x[i] + 1) % 2; result = 1; } } return result; } // This is an (1+1)_EA with static mutation rate = 1/n. void evolutionary_algorithm(std::shared_ptr&lt;IOHprofiler_problem&lt;int&gt;&gt; problem) { std::vector&lt;int&gt; x; std::vector&lt;int&gt; x_star; std::vector&lt;double&gt; y; double best_value; double mutation_rate = 1.0/problem-&gt;IOHprofiler_get_number_of_variables(); x = Initialization(problem-&gt;IOHprofiler_get_number_of_variables()); copyVector(x,x_star); y = problem-&gt;evaluate(x); best_value = y[0]; while(!problem-&gt;IOHprofiler_hit_optimal()) { copyVector(x_star,x); if(mutation(x,mutation_rate)) { y = problem-&gt;evaluate(x); } if(y[0] &gt; best_value) { best_value = y[0]; copyVector(x,x_star); } } } With a configuration of suite and logger by a file configuration.ini (see details ), the experiment can be run as: void _run_experiment() { std::string configName = &quot;./configuration.ini&quot;; IOHprofiler_experimenter&lt;int&gt; experimenter(configName,evolutionary_algorithm); experimenter._run(); } Configuration file Configuration.ini consists of three parts: [suite], [observer] and [triggers]. [suite] is the session that collects problems to be tested in the experiment. suite_name: Currently, ONLY PBO suite is avaiable, please do not modify the value of suite_name, unless a new suite is created. problem_id: presents id of problems of the suite. The format of function_id can be 1,2,3,4 using comma , to separate problems’ id, or be 1-4 using an en-dash - to present the range of problems’ id. instance_id: presents id of instances. Instanes 1 means there is no transformer operations on the problem. For instances 2-50, XOR and SHIFT operations are applied on the problem. For instances 5-100, SIGMA and SHIFT operations are applied on the problem. Larger instances ID will be considered as 1. dimension: presents dimensions of problems. The format of dimensions is as 500,1000,1500. [observer] is about the setting of output files. observer_name: Currently, ONLY PBO observer is avaiable, please do not modify the value of observer_name, unless a new observer is created. result_folder: Directory where stores output files. algorithm_name: used for .info files. algorithm_info: user for .info files. parameters_name: names for recording parameters in algorithms. [triggers] are parameters for different output files, see documentation to know technique of recording evluations. number_target_triggers, base_evaluation_triggers: are for .tdat files. complete_triggers: is for .cdat files. number_interval_triggers: is for .idat files. complete_triggers is the switch of .cdat files, which will store evaluations of all iterations. Set complete_triggers as TRUE or true if you want to output .cdat files update_triggers is the switch of .dat files, which will store evaluations when a better solution is found. Set complete_triggers as TRUE or true if you want to output .dat files number_interval_triggers works on the *.idat files. *.idat files log evaluations in a fixed frequecny. number_target_triggers sets the value of the frequecny. If you do not want to generate .idat files, set number_target_triggers as 0. both number_target_triggers and base_evaluation_triggers effect .tdat. number_target_triggers is a value defines the number of evaluations to be logged between 10^i and 10^(i+1). If you do not want to generate .tdat files, set number_target_triggers as 0. base_evaluation_triggers defines the base evaluations used to produce an additional evaluation-based logging. For example, if base_evaluation_triggers = 1,2,5, the logger will be triggered by evaluations $1n10^i, 2n*10^i, 5n10^i$ . If you do not want to generate .tdat files, set base_evaluation_triggers as ``. Getting started by Python Getting started by R",
    "url": "http://localhost:4000/IOHexperimenter/GettingStarted/",
    "relUrl": "/IOHexperimenter/GettingStarted/"
  },
  "4": {
    "id": "4",
    "title": "Graphic User Interface",
    "content": "Graphic User Interface Data loading Fixed-target results Expected runtime Probability mass function Cumulative distribution A web-based GUI is implemented in IOHanalyzer such that the user can analyze the experiment data easily in the web browser using two R packages: shiny and plotly. In this section, we shall describe this Graphical User Interface (GUI) in detail. Moreover, for the users who are not familiar with R , a free GUI server is hosted online (For any bug report, feedback or requested change, the users are encouraged to contact us through iohprofiler@liacs.leidenuniv.nl). Alternatively, invoking the GUI is also straightforward on your local machine, given package IOHanalyzer is already installed: &gt; runServer() Loading required package: shiny Listening on http://127.0.0.1:3943 which will start the GUI server on the local machine (hence using IP address 127.0.0.1 and a random port number). The web browser will be launched and connect to this address immediately after starting the server. There are tour groups of functionalities on the page. Upload Data: This section provides functionality to upload experiment results or load an official data set that is provided by the author. Fixed-Target Results This section provides statistics covering the fixed-target perspective of performance evaluation. That is, the results in this section mainly address the question about the statistical property of running time (i.e., function evaluations) that is needed to obtain a solution of a desired target quality. Fixed-Budget Results This sectioncovers the fixed-budget perspective, that is the statistics on objective function values obtained by the search points a given budget of function evaluations. In other words, the results in this section mainly address the question how good the search points are that a user can expect to see within a given frame of running time. Algorithm Parameters This section provides details about the evolution of the algorithm parameters that the user specified to be tracked in the experimental procedure. Data loading The GUI interface to load the experiment data is shown in the following figure, in which the user is asked to upload a compressed archive. The following compression format are supported: *.zip, *.bz, *.tar, *.xz, *.gz. Note that, when the user’s data set is enormous to handle, it is possible to speed up the uploading (and hence plotting) procedure by toggling option Efficient mode on, in which a subset is taken from the huge data set. Moreover, when using the online GUI, the user can also load official data sets provided by the author, using the Load Data from Repository box on the right of the page. At the time of writing, two official data sets are made available, each of which contains results of 11 algorithms on all 23 test functions, over dimensions {16, 100, 625}. For the specification of those two data sets and updates on the data set, the user is suggested to visit data page. Fixed-target results The fixed-target section has four different subsections: Data Summary Expected Runtime Probability Mass Function Cumulative Distribution. The analysis provided in those sections are described as follows: Data Summary provides some statistics on the running time $T(A, f, d, v)$, meaning the function evaluation an algorithm $A$ would require to reach target value $v$ of test function $f$ on dimension $d$. In the following, the indexing parameter in $T$ will be dropped if no ambiguity is created. Assuming a number of independent runs of algorithm $A$ is performed on the tuple of $(f, d, v)$, the set of results from all runs $(t_i)_i$ is considered as a simple random sample of $T$. Here, three tables are provided to summarize the sample $(t_i)_i$ of $T$: Data Overview A screenshot of this table is given as below. As counterintuitive as it may seem, this table contains the overview of the function value observed in a data set. The main reason of showing this table here is to provide the user a quick summary of the range of function value, which is required to play with the following two functionalities. Runtime Statistics at Chosen Target Values A screenshot of this table is given as below. The table is obtained from The user can set the range and the granularity of the results in the box on the left. The table shows fixed-target running times for evenly spaced target values. More precisely, for each tuple of (algorithm $A$, target value $v$, dimension $d$) the table provides 1) successful runs: the number of runs (sample points) of algorithm $A$ in which at least one solution $x$ satisfying $f(x)&gt;v$ has been found 2) sample mean, median, standard deviation 3) sample quantiles: $Q_{2 %}, Q_{5 %}, ldots, Q_{98 %}$ and 4) the expected running time (ERT). Additionally, the user can also download this table in CSV format, or as a LaTeX table. Original Runtime Samples The user interface is similar to the previous except that the sample points $(t_i)_{i}$ of running time $T(A, f, d, v)$ is shown here. Moreover, the user can choose between a long (all sample points are stored in a column) and a wide format (all sample points are stored in a row) for the table. Expected runtime An interactive plot (using shiny package) illustrates the fixed-target running times. An example of this plot is shown as below as below. The interactive plot can be adjusted by a couple of options on the left menu as shown in figure, including showing/hiding mean and/or median values along with standard deviations and scaling axis logarithmically. The user also selects the algorithms to be displayed, the range of target values within which the curves are drawn. The displayed curves can be switched on and off by clicking on the legend on the right of the plot. In addition, the figure can be downloaded in the following format: pdf, eps, svg and png (which also applies to all the plots hereafter). Probability mass function For a selected target value $v$, the histogram of the running time, as displayed below, shows the number of runs $i$ where the running time falls into a given interval $[t,t+1)$, namely $t le t_i(A,f,d,v) &lt; t+1$. The bin size $[t,t+1)$ is automatically determined according to the so-called Freedman-Diaconis rule, which is based on the interquartile range of sample $(t_i)_i$. The user has two options: overlayed display, where all algorithms are displayed in the same plot separated one, where each algorithm is displayed in an individual sub-plot. In addition to the histogram, the empirical probability mass function (see figure below) might be helpful to get a finer look at the shape of the empirical distribution of $T(A,f,d,v)$. The user can opt to show all sample points $(t_i)_{i}$ for each algorithm (which will be plotted as dots), or only the empirical probability mass function itself. It is important to point out that the probability mass function is estimated in a ““continuous’’ manner, where running time samples are considered as $ mathbb{R}$-valued and then a Kernel Density Estimation (KDE) is taken to obtain the curve. Note also that in the figure many sample points seem overlapping, this might be caused by turning (in the data upload part) the efficient mode on, in which the raw data set is trimmed. Cumulative distribution The empirical cumulative distribution function of the running time are computed for target values specified by the user. In addition to showing ECDFs for a single target value, it is recommended to aggregate ECDFs over multiple target, to obtain an overall performance profile for all algorithms. Such a functionality is exemplified below, a set of evenly spaced target value can be generated by specifying the range and step of the target value. In this example, with the following setup, $f_{ min}=0.46$, $f_{ max}=4.91$, and $ Delta f=0.5$, the ECDF curves for target values $0.46,0.96,1.46, ldots, 4.91$ are computed. The aggregation across targets is defined in the following sense: for a set of target values $V$, $r$ number of independent runs on each function, the aggregated ECDF considers running time samples of all target values and runs together. In the upper figure, for algorithm $(30,30)$-vGA (blue curve) this is the case for around $70 %$ of the pairs after $t=2 ,0000$ function evaluations. For algorithm RLS (purple curve) the fraction is $80 %$. Ideally, the best algorithm would sample the maximal function value $f_{ max}$ in the first function evaluation. This algorithm would have a 100% cumulative probability for any running time. In practice, such an algorithm does not exist, but it serves as a theoretical upper bound. Furthermore, this way of performing the aggregation can be leveraged to a set of test functions, namely it is straightforward to define the aggregated profile for a range of functions This functionality is also materizalized in IOHanalyzer. In the example figure, a table of pre-calculated target values are provided for each test function while all 23 test functions are considered here by default. This table could also be customized by 1) downloading the current table (in a CSV-like format) 2) modifying it according to user’s preference 3) uploading the modified table again. The plot on the right will be re-computed upon uploading a new table of targets. Note that, please keep the format of the example table while editing it.",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/GraphicUserInterface/",
    "relUrl": "/IOHanalyzer(Post-Processing)/GraphicUserInterface/"
  },
  "5": {
    "id": "5",
    "title": "IOHanalyzer",
    "content": "IOHanalyzer In this page, an example on IOHanalyzer is given based on the benchmark data generated in the previous section. In case it occurs to be time-consuming for the reader to execute the benchmarking example, the exactly same data set can also be downloaded from the site. We provide and maintain two versions of the IOHanalyzer package: a CRAN (Comprehensive R Archive Network) version R package a latest stable version that is hosted on Github page. The CRAN version can be obtained by: &gt;install.packages(&#39;IOHanalyzer&#39;) To install the latest stable version, pkg{devtools} is required again: &gt;devtools::install_github(&#39;IOHprofiler/IOHanalyzer&#39;) And please import the package after installation: &gt;library(&#39;IOHanalyzer&#39;) Note that those two versions only differ in some aesthetic aspects of the plotting method, which undergos a continuous and constant improvement. The example here is mainly two-fold: the usage of the programming interface and the Graphical User Interface.",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/",
    "relUrl": "/IOHanalyzer(Post-Processing)/"
  },
  "6": {
    "id": "6",
    "title": "IOHexperimenter",
    "content": "IOHexperimenter IOHexperimenter is a platform for testing iterative heuristic algorithms. It supplies a set of benchmark problems, allows users to test algorithms with well-desinged and customized output data format, and supports defining new test problems. Working with IOHanalyzer, users can easily generate output of algorithms with IOHexperimenter and analyze with IOHanalyzer The source code of IOHexperimenter is programmed by C++, and interface for Python, Java, R will be release soon.",
    "url": "http://localhost:4000/IOHexperimenter/",
    "relUrl": "/IOHexperimenter/"
  },
  "7": {
    "id": "7",
    "title": "Problems",
    "content": "Problems Definition F1: OneMax (Hamming Distance） It asks to optimize $OM:{0,1} rightarrow [0..n], x mapsto sum_{i=1}^n{x_i}$. The problem has a very smooth and non-deceptive fitness landscape. Due to the well-known coupon collector effect, it is relatively easy to make progress when the function values are small, and the probability to obtain an improving move decreases considerably with increasing function value. F2: LeadingOnes The problem asks to maximize the function $LO:{0,1}^n to [0..n], x mapsto max {i in [0..n] mid forall {j} le {i}: x_j=1} = sum_{i=1}^n{ prod_{j=1}^i{x_i}}$, which counts the number of initial ones. F3: A Linear Function with Harmonic Weights The problem is a linear function $f:{0,1}^n to mathbb{R}, x mapsto sum_{i} i x_i$ with harmonic weights. F4-F17: The W-model The W-model comprises 4 basic transformations, each coming with different instances. We use $W( cdot, cdot, cdot, cdot)$ to denote the configuration chosen in our benchmark set. Reduction of dummy variables $W(k, ast, ast, ast)$: a reduction mapping each string $(x_1, ldots, x_n)$ to a substring $(x_{i_1}, ldots, x_{i_k})$ for randomly chosen, pairwise different $i_1, ldots, i_k in [n]$. Neutrality $W( ast, mu, ast, ast)$: The bit string $(x_1, ldots,x_n)$ is reduced to a string$(y_1, ldots,y_m)$ with $m:=n/ mu$, where $ mu$ is a parameter of the transformation. For each $i in [m]$ the value of $y_i$ is the majority of the bit values in a size-$ mu$ substring of $x$. More precisely, $y_i=1$ if and only if there are at least $ mu/2$ ones in the substring $(x_{(i-1) mu+1},x_{(i-1) mu+2}, ldots,x_{i mu})$. When ${n/ mu} notin { mathbb{N}}$, the last bits of $x$ are simply copied to$y$. In our assessment, we regard only the case $ mu=3$. Epistasis $W( ast, ast, nu, ast)$ The idea of epistasis is to introduce local perturbations to the bit strings. To this end, a string $x=(x_1, ldots,x_n)$ is divided into subsequent blocks of size $ nu$. Using a permutation $e_{ nu}:{0,1}^{ nu} to {0,1}^{ nu}$, each substring $(x_{(i-1) nu+1}, ldots,x_{i nu})$ is mapped to another string $(y_{(i-1) nu+1}, ldots,y_{i nu})=e_{ nu}((x_{(i-1) nu+1}, ldots,x_{i nu}))$. The permutation $e_{ nu}$ is chosen in a way that Hamming-1 neighbors $u,v in {0,1}^{ nu}$ are mapped to strings of Hamming distance at least $ nu-1$. In our evaluation, we use $ nu=4$ only. Fitness perturbation $W( ast, ast, ast,r)$. With this transformation we can determine the ruggedness and deceptiveness of a function. Unlike the previous transformations, this perturbation operates on the function values, not on the bit strings. To this end, a emph{ruggedness} function $r:{f(x) mid {x} in {0,1}^n }=:V to {V}$ is chosen. The new function value of a string $x$ is then set to $r(f(x))$ , so that effectively the problem to be solved by the algorithm becomes ${r} circ {f}$. We use the following three ruggedness functions. $r_1:[0..s] to [0.. lceil{s/2} rceil+1$ with $r_1(s)= lceil {s/2} rceil +1$ and $r_1(i)= lfloor {i/2} rfloor+1$ for $i&lt;s$ and even s, and $r_1(i)= lceil {i/2} rceil+1$ for $i&lt;s$ and odd $s$. This function maintains the order of the search points $r_2:[0..s] to [0..s]$ with $r_2(s)=s$, $r_2(i)=i+1$ for $i equiv {s {mod} 2}$ and $i&lt;s$, and $r_2(i)= max{ {i-1,0}}$ otherwise. This function introduces moderate ruggedness at each fitness level. $r_3:[0..s] to [0..s]$ with $r_3(s)=s$ and $r_3(s-5j+k)=s-5j+(4-k)$ for all $j in {[s/5]}$ and $k { in} [0..4]$ and $r_3(k)=s - (5 lfloor {s/5} rfloor - 1 )- k$ for $k in [0..s - 5 lfloor {s/5} rfloor -1]$. With this function the problems become quite deceptive, since the distance between two local optima implies a difference of $5$ in the function values. F4-F17 present superpositions of individual W-model transformations to the OneMax (F1) and the LeadingOnes (F2) problem. Precisely, F4-F17 are F4: $ text{OneMax} + W([n/2],1,1,id)$ F5: $ text{OneMax} + W([0.9n],1,1,id)$ F6: $ text{OneMax} + W([n], mu=3,1,id)$ F7: $ text{OneMax} + W([n],1, nu=4,id)$ F8: $ text{OneMax} + W([n],1,1,r_1)$ F9: $ text{OneMax} + W([n],1,1,r_2)$ F10: $ text{OneMax} + W([n],1,1,r_3)$ F11: $ text{LeadingOnes} + W([n/2],1,1,id)$ F12: $ text{LeadingOnes} + W(0.9n,1,1,id)$ F13: $ text{LeadingOnes} + W([n], mu=3,1,id)$ F14: $ text{LeadingOnes} + W([n],1, nu=4,id)$ F15: $ text{LeadingOnes} + W([n],1,1,r_1)$ F16: $ text{LeadingOnes} + W([n],1,1,r_2)$ F17: $ text{LeadingOnes} + W([n],1,1,r_3)$ F18: Low Autocorrelation Binary Sequences (LABS) The Low Autocorrelation Binary Sequences (LABS) problem poses a non-linear objective function over a binary sequence space, with the goal to maximize the reciprocal of the sequence’s autocorrelation: $ frac{n^2}{2E(S)} text{ with } E(S) = sum_{k=1}^{n-1} left( sum_{i=1}^{n-k}s_i cdot s_{i+k} right)^2$ where the sequence is of length n $S:= left(s_1, ldots,s_n right)$ with $s_i= pm 1$ . To obtain a pseudo-Boolean problem, we use the straightforward interpretation $s_i=2x_i-1$ for all $i in [n]$. F19-F21: The Ising Model The classical Ising model cite{Ising_Barahona1982} considers a set of spins placed on a regular lattice $G=([n],E)$, where each edge $(i,j) in {E}$ is associated with an interaction strength $J_{ij}$. Given a configuration of $n$ spins, $S:= left(s_1, ldots,s_n right)$, this problem poses a quadratic function, representing the system’s energy and depending on its structure $J_{ij}$. Assuming zero external magnetic fields and using $s_i=2x_i-1$ we obtain the following pseudo-Boolean maximization problem: $[ISING:] sum limits_{ {i,j} in {E}} left[x_{i}x_{j} - left(1-x_{i} right) left(1-x_{j} right) right] $ F19 considers the one-dimensional ring, F20 considers the two-dimensional torus, and F21 considers the two-dimensional triangular. F22: Maximum Independent Vertex Set Given a graph $G=([n],E)$ , an independent vertex set is a subset of vertices where no two vertices are linked by an edge. A maximum independent vertex set (MIVS) is defined as an independent subset Given a graph $V^{ prime} subset [n]$ having largest possible size. $[MIVS:] sum_i x_i ; textrm{s.t.} ; sum_{i &lt; j} x_i x_j e_{ij} = 0~$. F23: N-Queens Problem The N-queens problem (NQP) is defined as the task to place N queens on an ${N} times{N}$ chessboard in such a way that they cannot attack each other. Selection of upper problems is suggested in the paper https://doi.org/10.1145/3319619.3326810 .",
    "url": "http://localhost:4000/Benchmark/Problems/",
    "relUrl": "/Benchmark/Problems/"
  },
  "8": {
    "id": "8",
    "title": "Programming Interface",
    "content": "Programming Interface Data structure and manipulation Retrieving performance data Plotting Data structure and manipulation Here, it is assumed that the data to be loaded follow exactly the aforementioned format regulation. A method DataSetList is provided to load the data: &gt; dsList &lt;- DataSetList(&#39;./data/RLS&#39;) Processing ./data/RLS/IOHprofiler_f1_i1.info ... algorithm RLS... 25 instances on f1 16D... 25 instances on f1 100D... ... The return value of method DataSetList is a S3 object, that is inherited from the list class. Consequently, list object can be sliced, indexed and printed as with lists: &gt; dsList DataSetList: 1: DataSet(RLS on f1 16D) 2: DataSet(RLS on f1 100D) 7: DataSet(RLS on f23 16D) 8: DataSet(RLS on f23 100D) &gt; dsList[1:3] DataSetList: 1: DataSet(RLS on f1 16D) 2: DataSet(RLS on f1 100D) 3: DataSet(RLS on f19 16D) &gt; dsList[[1]] DataSet(RLS on f1 16D) In addition, the summary method is implemented to show some basic information: &gt; summary(dsList) funcId DIM algId datafile comment 1 1 16 RLS ./data/RLS/data_f1/IOHprofiler_f1_DIM16_i1.dat % 2 1 100 RLS ./data/RLS/data_f1/IOHprofiler_f1_DIM100_i1.dat % 3 19 16 RLS ./data/RLS/data_f19/IOHprofiler_f19_DIM16_i1.dat % 4 19 100 RLS ./data/RLS/data_f19/IOHprofiler_f19_DIM100_i1.dat % 5 2 16 RLS ./data/RLS/data_f2/IOHprofiler_f2_DIM16_i1.dat % 6 2 100 RLS ./data/RLS/data_f2/IOHprofiler_f2_DIM100_i1.dat % 7 23 16 RLS ./data/RLS/data_f23/IOHprofiler_f23_DIM16_i1.dat % 8 23 100 RLS ./data/RLS/data_f23/IOHprofiler_f23_DIM100_i1.dat % Note that, column funcId stands for the numbering (ID) of test functions and algId is the identifier of the algorithm that is tested. Those columns (also DIM) are the attribute of list object, which are directly retrieved from the meta data (*.info files). Therefore, it is important to keep the meta data correct if it were prepared by the user manually. All the attributes DataSetList object are listed as follows: &gt; attributes(dsList) $class [1] &quot;list&quot; &quot;DataSetList&quot; $DIM [1] 16 100 16 100 16 100 16 100 $funcId [1] 1 1 19 19 2 2 23 23 $algId [1] &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; &quot;RLS&quot; When subsetting (filtering) is needed for list, attributes DIM, funcId and algId can be used as follows: &gt; subset(dsList, DIM == 16, funcId == 1) DataSetList: 1: DataSet(RLS on f1 16D) &gt; subset(dsList, DIM == 16, algId != &#39;RLS&#39;) DataSetList: Now we could load the data files of the $(1, lambda)$-GA algorithm in the same way: &gt; dsList_ga &lt;- DataSetList(&#39;./data/self_GA&#39;, verbose = FALSE) Here, the argument verbose is set to FALSE to hide the prompting message. As with the R list, DataSetList objects can be combined together: &gt; dsList &lt;- c(dsList, dsList_ga) Each element of list is a S3 object of type DataSet, which is again inherited from the list class. &gt; ds &lt;- dsList[[1]] &gt; ds DataSet(RLS on f1 16D) &gt; summary(ds) DataSet Object: Algorithm: RLS Function ID: 1 Dimension: 16D 25 instance found: 1,1,1,1,1,2,2,...,4,4,4,5,5,5,5,5 runtime summary: algId target mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% ERT runs ps 1: RLS 4 1.00 1 0.000000 1 1 1 1 1 1 1 1 1 1.00 25 1 2: RLS 5 1.04 1 0.200000 1 1 1 1 1 1 1 1 1 1.04 25 1 3: RLS 6 1.28 1 1.208305 1 1 1 1 1 1 1 2 2 1.28 25 1 11: RLS 14 21.00 21 9.165151 5 5 7 14 18 26 34 37 37 21.00 25 1 12: RLS 15 29.16 29 10.466932 12 12 15 18 26 36 40 48 48 29.16 25 1 13: RLS 16 46.48 48 21.652790 13 13 19 26 42 58 71 83 83 46.48 25 1 function value summary: algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: RLS 1 25 7.68 7 1.7729448 4.48 5.2 6.0 7 7 9 10.0 10.8 11.00 2: RLS 2 25 8.28 8 1.6206994 5.48 6.0 6.4 7 8 9 10.0 10.8 11.52 3: RLS 3 25 8.76 9 1.5885003 5.48 6.2 7.0 8 9 10 10.6 11.0 11.52 64: RLS 2511 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 65: RLS 2818 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 66: RLS 3162 25 16.00 16 0.0000000 16.00 16.0 16.0 16 16 16 16.0 16.0 16.00 Attributes: names, class, suite, funcId, DIM, algId, algInfo, comment, datafile, instance, maxRT, finalFV, format, maximization In the summary method, the data set is summarized in two perspectives: Fixed-target perspective: the method looks for the first hitting time, that is the number of function evaluations an algorithm takes to reach a target function value (target above) for the first time. The target values are automatically determined and evenly spaced in the observed range. Some basic statistics on the running time sample are calculated for each target value: mean, median, standard deviation (sd), percentiles (2% 5% 10% …), the expected running time (ERT) and success rate (ps, the ratio of successful runs our of all the independent runs). Fixed-budget perspective: the method looks for the best function value reached by the algorithm, when a specific number of function evaluations (budget) are taken (runtime above). The budget values are automatically determined and evenly spaced in the observed range. Roughly the same set of statistics are provided as the fixed-target perspective. In the ds object, two matrices are always stored for those two perspectives explained above: ds$RT: running time samples in the fixed-target perspective and ds$FV: function value samples in the fixed-budget perspective. Note that, when the parameter tracking is enabled, the parameter of interest is also arranged in the fixed-target perspective and is appended to ds object. For instance, if mutation_rate is the parameter name given to the benchmark, the parameter can be obtained by ds$mutation_rate. We could take a glimpse at those two matrices: &gt; head(ds$RT) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 1 1 1 1 2 1 1 6 1 1 1 1 1 1 1 1 2 1 1 7 1 1 7 1 1 1 1 1 3 1 1 5 1 1 8 1 1 8 1 3 2 2 1 4 1 1 7 2 1 10 2 1 9 1 4 3 3 4 5 1 1 8 4 2 11 3 1 Here, the column names are the target values and in the example below, those are the budget values: &gt; head(ds$FV) [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] 1 11 7 7 7 8 6 9 9 5 7 8 4 7 11 2 11 7 8 8 8 6 9 10 6 8 9 5 8 12 3 11 8 9 9 8 7 10 10 6 8 9 5 9 12 4 12 9 10 9 9 8 10 11 6 9 10 5 9 13 5 12 9 10 9 10 9 10 12 7 10 10 5 10 14 6 13 9 11 10 10 10 10 13 7 11 11 5 10 14 Retrieving performance data For both DataSet and DataSetList objects, the overview of the observed running time/function value can obtained using: &gt; get_RT_overview(subset(dsList, algId == &#39;RLS&#39;)) Algorithm DIM fID miminal runtime maximal runtime runs Budget 1: RLS 16 1 1 96 25 16000 2: RLS 100 1 1 1128 25 100000 3: RLS 16 19 1 52 25 16000 4: RLS 100 19 1 570 25 100000 5: RLS 16 2 1 255 25 16000 6: RLS 100 2 1 6546 25 100000 7: RLS 16 23 1 67 25 16000 8: RLS 100 23 1 641 25 100000 &gt; get_RT_overview(ds) algId DIM funcId miminal runtime maximal runtime runs Budget 1: RLS 16 1 1 96 25 16000 Here, Budget indicates the maximal allowable budget that is given when running the experiment while maximal runtime is the maximal observed running time in each triplet of (algId, DIM, funcId). For the function values, the similar methods are also implemented: &gt; get_FV_overview(subset(dsList, algId == &#39;RLS&#39;)) algId DIM funcId worst recorded worst reached best reached mean reached median reached runs succ budget 1: RLS 16 1 4 16 16 16.00 16 25 25 16000 2: RLS 100 1 44 100 100 100.00 100 25 25 100000 3: RLS 16 19 8 20 32 25.60 24 25 1 16000 4: RLS 100 19 92 152 172 162.56 164 25 3 100000 5: RLS 16 2 0 16 16 16.00 16 25 25 16000 6: RLS 100 2 0 100 100 100.00 100 25 25 100000 7: RLS 16 23 -100 3 4 3.24 3 25 6 16000 8: RLS 100 23 -1868 7 9 8.20 8 25 9 100000 It is important to distinguish some columns in the example here: worst recorded stands for the worst (smallest) function value observed in all independent runs for each case of (algId, DIM, funcId). In contrast, worst reached means the smallest value reached in the last iteration (across independent runs) of the algorithm while best reached the largest value. runs gives the total number of independent runs in each case while succ is the number of runs where the corresponding best reached is hit. Note that, in our naming convention of methods, RT is always the abbreviation of running time and FV is for function value (the same below). To get a data summary at arbitrary running time/function value, two methods, get_RT_summary and get_FV_summary are implemented. Let use the object ds (defined before) to illustrate the usage: &gt; ds DataSet(RLS on f1 16D) &gt; get_RT_summary(ds, ftarget = c(5, 10, 16)) algId target mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% ERT runs ps 1: RLS 5 1.04 1 0.200000 1 1 1 1 1 1 1 1 1 1.04 25 1 2: RLS 10 5.24 5 3.455431 1 1 1 2 5 6 10 12 12 5.24 25 1 3: RLS 16 46.48 48 21.652790 13 13 19 26 42 58 71 83 83 46.48 25 1 &gt; get_FV_summary(ds, runtime = c(10, 50, 100)) algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: RLS 10 25 11.52 12 1.5307950 8.48 9 9.4 11 12 13 13 13.8 14 2: RLS 50 25 15.64 16 0.4898979 15.00 15 15.0 15 16 16 16 16.0 16 3: RLS 100 25 16.00 16 0.0000000 16.00 16 16.0 16 16 16 16 16.0 16 The input for the argument ftarget and runtime should be provided by the user. In this example, three values are chosen arbitrarily in the corresponding range of running time/function value (cf. the first lines of get_RT_overview and get_FV_overview above). Furthermore, using the magrittr package (please install it if necessary), it is possible to chain all the methods introduced so far, making the code snippet more readable: &gt; library(magrittr) &gt; dsList %&gt;% + subset(DIM == 100, algId == &#39;RLS&#39;, funcId == 19) %&gt;% + get_FV_summary(runtime = seq(1, 5000, length.out = 5)) DIM funcId algId runtime runs mean median sd 2% 5% 10% 25% 50% 75% 90% 95% 98% 1: 100 19 RLS 1.00 25 104.16 104 8.284926 92 92.0 93.6 100 104 108 116.8 120 120 2: 100 19 RLS 1250.75 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 3: 100 19 RLS 2500.50 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 4: 100 19 RLS 3750.25 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 5: 100 19 RLS 5000.00 25 162.56 164 6.096994 152 152.8 156.0 156 164 168 170.4 172 172 In addition, it is also straightforward to retrieve the raw sample points of the running time/function value, using the following methods, get_RT_sample and get_FV_sample: &gt; get_RT_sample(ds, ftarget = 10, output = &#39;long&#39;) algId target run RT 1: RLS 10 1 1 2: RLS 10 2 8 3: RLS 10 3 4 23: RLS 10 23 12 24: RLS 10 24 1 25: RLS 10 25 8 &gt; get_FV_sample(ds, runtime = c(5, 20), output = &#39;wide&#39;) algId runtime run.1 run.2 run.3 run.4 run.5 run.6 run.7 run.8 run.9 run.10 run.11 run.12 1: RLS 5 12 9 10 9 10 9 10 12 7 10 10 5 2: RLS 20 16 12 15 14 14 13 14 15 14 12 14 13 Note that, all methods in this sub-section return a data.table object from the data.table package. Plotting To visualize the benchmark data, a collection of plotting methods are implemented in iohana. In this section, we shall provide examples on some important plots using the data set in the last section. Here, only the plotting method for the fixed-target perspective is shown because the same set of methods are implemented for the fixed-budget perspective. Firstly, the progression of the function value is plotted against the running time: &gt; ds_plot &lt;- subset(dsList, DIM == 16, funcId == 1) &gt; Plot.RT.Single_Func(ds_plot) The data sets on 16D, function F1 are plotted here, which is shown in the following figure. Note that, a interactive plot is created as the plotly library is used here by default. The static plotting library ggplot2 can also be selected by setting argument backend = &#39;ggplot2&#39; (this is only a difference in the plotting backend and thus it will not be demonstrated here). In the figure, the standard deviation of the running time is also drawn. &gt; ?Plot.RT.Single_Func In addition, the previous plot can be grouped by functions, using Plot.RT.Multi_Func methods. The example is shown below. &gt; Plot.RT.Multi_Func(ds_plot, scale.ylog = T) Given a target value, Plot.RT.Histogram methods renders the histogram of the running time required to reach this target value. Taking the data set on F23 and 16D as an example, it is important to view the range of function values through method get_FV_overview as shown in the previous section: &gt; ds &lt;- subset(dsList, DIM == 16, funcId == 23) &gt; get_FV_overview(ds) algId DIM funcId worst recorded worst reached best reached ... Budget 1: RLS 16 23 -100 3 4 ... 16000 2: self_GA 16 23 -96 4 4 ... 16001 Then, we choose a target value -3 that is close to the best reached value and plot the histogram. &gt; Plot.RT.Histogram(ds, ftarget = -3, plot_mode = &#39;subplot&#39;) The argument plot_mode = &#39;subplot&#39; will create a separate sub-plot for each algorithm in the data set. In addition, the empirical density function of the running time, that is estimated by the Kernel Density Estimation (KDE) method, can be generated by method Plot.RT.PMF. &gt; Plot.RT.PMF(ds, ftarget = -3, show.sample = TRUE) Finally, it is also crucial to look at the Empirical Cumulative Distribution function (ECDF) of the running time. For this purpose, three methods are implemented for different levels of data aggregation: Plot.RT.ECDF_Per_Target: it only compares the ECDF of algorithms on a single target value, e.g., &gt; Plot.RT.ECDF_Per_Target(ds, ftarget = -1) Plot.RT.ECDF_Single_Func: it takes as input an array of target values (controlled by arguments fstart, fstop, fstep) and aggregates the ECDF over those targets, e.g., &gt; Plot.RT.ECDF_Single_Func(ds, fstart = -92, fstop = 4, fstep = 10) Plot.RT.ECDF_Multi_Func: it, in addition, aggregates different target values over all test function in a data set. To demonstrate its usage, let’s take the data set on 100D and check the overview of the function values. Then three target values are chosen manually for each function, which are collected in a list object. The resulting plot is shown i. &gt; ds &lt;- subset(dsList, DIM == 100) &gt; get_FV_overview(ds) algId DIM funcId worst recorded worst reached best reached ... budget 1: RLS 100 1 44 100 100 ... 100000 2: RLS 100 19 92 152 172 ... 100000 3: RLS 100 2 0 100 100 ... 100000 4: RLS 100 23 -1868 7 9 ... 100000 5: self_GA 100 1 38 98 100 ... 100001 6: self_GA 100 19 72 164 192 ... 100001 7: self_GA 100 2 0 39 100 ... 100001 8: self_GA 100 23 -1761 7 10 ... 100001 &gt; ftarget &lt;- list(`1` = c(80, 90, 100), + `2` = c(80, 90, 100), + `19` = c(180, 190, 200), + `23` = c(0, 5, 10)) &gt; Plot.RT.ECDF_Multi_Func(ds, ftarget)",
    "url": "http://localhost:4000/IOHanalyzer(Post-Processing)/ProgrammingInterface/",
    "relUrl": "/IOHanalyzer(Post-Processing)/ProgrammingInterface/"
  },
  "9": {
    "id": "9",
    "title": "Transformation",
    "content": "Transformation Instead of testing one particular problem $f$ only, the user can choose to run experiments on several problem instances that are obtained from $f$ through a set of transformations. In its most general form,IOHprofiler currently offers to return to the algorithm the values as $af( sigma (x { oplus} z)) + b$, where $a$ is a multiplicative shift of the function value, $b$ is an additive shift of the function value, ${ oplus}z: {0,1}^n to {0,1}^n, (x_1, … ,x_n) mapsto ((x_1 + z_1) {mod} 2, … ,(x_n + z_n) {mod} 2)$ is an XOR-shift of the search point, $ mathcal{S}^n to mathcal{S}^n, (x_1, … ,x_n) mapsto (x_{ sigma(1)}, … ,x_{ sigma(n)})$ is a permutation of the search point. Note here that, in abuse of notation, we identify the permutation ${ sigma}: [1..n] to [1..n]$ with the here-defined re-ordering of the bit string. In practical, we applied $af(x { oplus} z) + b$ to instance 2-50, and $af( sigma (x)) + b$ to instance 51-100. $a in [ frac{1}{5},5]$ and $b in [-1000,1000]$ are randomly generated.",
    "url": "http://localhost:4000/Benchmark/Transformation/",
    "relUrl": "/Benchmark/Transformation/"
  },
  "10": {
    "id": "10",
    "title": "About",
    "content": "IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics IOHprofiler is a new tool for analyzing and comparing iterative optimization heuristics. Given as input algorithms and problems written in C, C++, or Python*, it provides as output an in-depth statistical evaluation of the algorithms’ fixed-target and fixed-budget running time distributions. In addition to these performance evaluations, IOHprofiler also allows to track the evolution of algorithm parameters, making our tool particularly useful for the analysis, comparison, and design of (self-)adaptive algorithms. IOHprofiler is a ready-to-use software. It consists of two parts: IOHexperimenter, which generates the running time data; and IOHanalyzer, which produces the summarizing comparisons and statistical evaluations. Source Both source code of IOHexperimenter and IOHanalyzer are available in github, IOHanalyze is also host online. Documentation IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics Contact If you have any questions, comments, suggestions or pull requests, please don’t hesitate contacting us IOHprofiler@liacs.leidenuniv.nl! Cite us",
    "url": "http://localhost:4000/about/",
    "relUrl": "/about/"
  },
  "11": {
    "id": "11",
    "title": "Home",
    "content": "IOHprofiler: A Benchmarking and Profiling Tool for Iterative Optimization Heuristics IOHprofiler is a new tool for analyzing and comparing iterative optimization heuristics. Given as input algorithms and problems written in C, C++, or Python*, it provides as output an in-depth statistical evaluation of the algorithms’ fixed-target and fixed-budget running time distributions. In addition to these performance evaluations, IOHprofiler also allows to track the evolution of algorithm parameters, making our tool particularly useful for the analysis, comparison, and design of (self-)adaptive algorithms. IOHprofiler is a ready-to-use software. It consists of two parts: IOHExperimenter, which generates the running time data; and IOHAnalyzer, which produces the summarizing comparisons and statistical evaluations. Currently IOHExperimenter is built on the COCO software, but a new C++ based version is developing and will be released soon. [This code] implements the experimentation tool of IOHprofiler. For the analyzer part, please visit IOHAnalyzer page.",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "12": {
    "id": "12",
    "title": "News",
    "content": "News Welcome to IOHprofiler",
    "url": "http://localhost:4000/News/",
    "relUrl": "/News/"
  }
  
}
